[
  
    {
      "title"    : "How to Actually Brainstorm (like a respectable businessperson)",
      "category" : "product",
      "content"	 : "How do you brainstorm while maintaining a balance between pragmatism and creativity? In this post, I’ll discuss some theory behind maintaining this balance, and then I’ll discuess some actual, real-world tips for how to “actually brainstorm”, and where to proceed next.Who is this for?This is mostly geared towards small groups, perhaps working in a startup environment. It revolves around a company structure that may have certain elements, like the business, engineering, and design team or individuals.We’ll start with a bit of theory, and then go into some practical methods to actually get something out of a brainstorming session.TL;DR  There are 2 extremes: overly pragmatic and overly naïve  There are bad ideas, but you don’t know which part of it is bad until you say it out loud  Lay the groundwork, have a facilitator, and warm it up  Ideas come in 3s: one idea is a bad idea, two ideas is an argument, and three ideas is brainstorming  Have a respectable plant who says bad ideas to unblock peopleIntroTypically, the term “brainstorming” brings to mind one of the following scenarios:  The boss calls a brainstorming session, and mostly people are quiet for fear of saying something stupid. A couple brave ideas might be written on a whiteboard. The meeting is adjourned with people feeling like they didn’t really come up with any good ideas.  Someone calls a brainstorming session, and everybody is throwing out random, unrealistic ideas. People are talking over one another; smaller groups form where independent ideas are discussed separately from the rest of the group. The meeting is adjourned with people feeling more confused than when they started.These 2 scenarios represent a couple of common extremes (one on either side of the spectrum): control, but no creativity, and creativity, but no control. The trick is to find a proper balance.Hang upsOn one side of the spectrum, there is the overly pragmatic person or people in the room who view brainstorming as a waste of time, and any ideas that are not completely realistic should be immediately suppressed before they are even said.Even further on that side of the spectrum, there are people who suppress even the notion of openly discussing new ideas, for fear that those who are discussing the ideas are planning to execute them.You should not conflate the two ideas, as they are completely separate things: open discussion of new ideas should not be interpreted to suggest that they will be implemented. This fear is valid, because you don’t want a “runaway” engineering or design team, and, contrary to the popular aphorism “there are no bad ideas”, there are bad ideas. But that doesn’t mean you shouldn’t say them out loud.Ideas are like faucets  “View it as a dirty tap. When you switch on the dirty tap on it’s going to flow shit water for a substantial amount of time. Then, clean water is going to start flowing. Every now and again you’re gonna get a bit of shit, but as long as you get it out of you, it’s fine. It’s the same thing with gigs. You will always play bad gigs in the beginning. The more gigs you do, the better you will get.”Ed Sheeran, one of the fastest growing artists in the world over the last couple years, said the above in regards to songwriting, as a way to express how he approaches songwriting.This translates nicely into our mental model for brainstorming as well: just get the idea out there. There are bad ideas, but it’s easier to understand which parts are good, and which parts are bad, if you actually say it out loud. Get the shit out of the way, and let the clean water flow.“Imagine the unimaginable, humor your imagination.”The above sentiment comes from Pete Blaber, a prior Delta Force commander, who participated in (and had great success with) Operation Anaconda. If you don’t know what any of this means, just know that it’s coming from a pretty badass guy who was in charge of autonomously leading a group of other badass guys in extremely dangerous circumstances, and he was really good at it.I know, I know: the comparison between business and special forces is an old, tired, annoying one. Bear with me for a second, though. Don’t worry, I’m not gonna call anybody a “design ninja” or a “coding warrior”.He wrote a book called “The Mission, The Men, and Me” that shares some of his philosophies. However, the one that I think is mostly relevant here (which is a lesson made available online) is in regards to brainstorming.  At the start of a new mission, the force is attempting to capture one of the most notorious war criminals on the UN’s most-wanted list, Osama Bin Ladin (UBL). Pete’s job is to collect any information on the environment and around the road that UBL will be traveling. They filmed the road in every possible angle so nothing could be hidden. The team gets together to try and figure out a concept to achieve surprise. One member thought to create an accident or woman looking for help, but they thought the criminal wouldn’t bother to slow down. Then someone thought of a gorilla suit. The team almost considered using a gorilla suit to capture UBL. The imagination that the team thought of was crazy, but brought the humor out to everyone. Pete believes that imagination created the evolution of man to dominate the planet to keep ourselves from extinction. Mankind found the ability to use fire, water, and wood by imagining. The side that usually wins the battle is the one that can out-smart and out-imagine the enemy as history has proven. The importance from this quote is that imagination drives the road to success. The imagination could then be turned into reality which can end in victory.The important part here is to allow yourself to come up with stupid ideas. This reinforces the idea that, yes, there are bad ideas, but there may be seeds of truth (or “good parts”) to some bad idea. But you need to say it first.How to actually brainstormStep 1: everybody on the same pageFirstly, I think the basis of any good brainstorming is that people are on the same page from the get-go. This means that everyone you are in the room with is familiar with the same theory behind what you’re trying to accomplish. They’ve been indoctrinated to the faucet analogy. They know the 2 extremes we discussed above, and they know how to avoid being overly pragmatic and overly naïve. It follows then, that brainstorming is both a uniquely individual and collective experience.Make sure everybody knows the basis from which they are operating.Step 2: have a facilitatorMake sure people know who is the facilitator. The facilitator’s job is not to be in charge of anything, nor decide anything about which ideas go up on the whiteboard, or which ones stay off. The facilitator simply directs the flow of things. This allows us to have a certain degree of structure, so that we don’t wander into “fluffy hipster territory”. This is a business, after all.This person will simply take ideas and write them on a whiteboard, or a window, or a piece of paper.Step 3: warm it upOnce everybody is on the same page and knows who the facilitator is, let’s then start talking. Lay down a goal: what are we trying to figure out?Even if everybody is on the same page, most people will fear that they will say something stupid. This fear is powerful, because it is directly tied to a person’s credibility.  “If I say stupid stuff in a brainstorming session, how can I then be trusted to do my job?”The idea of warming up means that we need to break the logical fallacy, and ensure that people unconsciously (or better yet, consciously) understand that saying something stupid here, does not mean they are stupid in other places of their lives. Again, break the conflation.There are a few tricks for this: ideas in threes, and a plant.The concept of ideas in threes, basically means that any time one idea is said, and no one is willing to give any other ideas, we must collectively force 2 other relevant ideas. One idea is a bad idea, two ideas is an argument, and three ideas is a brainstorm.In that article, another interesting thing is mentioned:  For some reason, bad ideas unblock people. My theory is that people are blocked because they’re trying to edit their ideas to things that perfectly meet their own idea of the requirements. That’s natural — people want to say smart things, not stupid things.This brings the concept of a plant. Ideally, this is somebody more senior in the group, who has some sort of real or perceived authority or respect in some aspect of what they do. It’s easy to do: this person just says with tongue in cheek, “hey guys, I’m gonna be the bad idea guy/girl. For the sake of getting all the bad ideas out of my system, I’m gonna go ahead and say one…”, and then proceeds with some off-the-wall, partially realistic idea. This can be said with a bit of a sly smile, so that people know you’re half-joking, but half-serious.However, the end result is that people realize the following:  “He knows he’s saying something stupid, but there’s a reason for it”  “If someone with authority and respect in the company is saying something stupid and isn’t shamed or criticized for it, surely I can say my idea, which isn’t nearly as stupid”  People laughPeople are now unblocked. You can use the plant throughout the entire brainstorming session. The idea is to unblock the faucet so that the dirty and clean water can flow out.Where to go nextOkay, brainstorm is over, now what? Well, now you can do lots of stuff:  You can rate the priority and quality of the ideas  Once you have these ratings, you can use this information in your next product planning meetingAside from that, now you have everybody on the same page for implementing possible features, etc. The idea here isn’t necessarily to have a strict plan; it’s more to have a framework for generating information that can be useful in the future.ConclusionRemember, you don’t need to fool yourself: there are bad ideas, but you need to say them out loud to find out which one are the good ones. Make sure everybody knows this when you’re brainstorming. It’s good to utilize some little tricks, like “ideas in threes” and “the plant” in order to unblock people.",
      "url"      : "/product/2017/08/15/how-to-actually-brainstorm.html",
      "date"     : "2017-08-15 12:45:00 +0000"
    },
  
    {
      "title"    : "Practical Queue Considerations",
      "category" : "rabbitmq",
      "content"	 : "Intuitively, we jump to HTTP to use as our communication protocol between services in a system. However, using a queue has many benefits. In this post, instead of diving only into theory of queues, I’ll give practical advice on implementing a queue, illustrate some use cases I’ve encountered in a production system where using a queue had obvious benefits versus HTTP, as well as give some practical implementation ideas.Who is this post for?  Those who have used or are familiar with the idea behind queues (like RabbitMQ), and have a general idea about the purpose and benefits/tradeoffs of a system that uses microservices.Recommended Reading  Enterprise Integration PatternsTL;DR  Tip: Use message types properly. Read Enterprise Integration Patterns to learn about message types (document, command, and event messages)… I cannot stress this point enough  Benefit: messages still get delivered even if a server goes down (in theory). But, make sure your consumers are idempotent.  Implementation idea: try using a queue for each service  Tip: set up a dead letter queue  Tip: encrypt everything yourselfMessage TypesEnterprise Integration Patterns was published back in 2004, but has invaluable information for patterns in developing a system that uses queues for communication. Best of all, the information is clear and practical (by the way, I don’t get anything for recommending this book, I just think it’s really good). The biggest question I had going into implementing a queue system was:How do I format my messages? (Book gives you the patterns: document, event, command)Very briefly:  Document message - data is a single unit of data. It is a piece of information (for example, a MongoDB or SQL document)  Event message - it’s simply a notification that something happened (*)  Command message - it is a verb that says “do this”I’ll give you an example of how these message types interact with one another:You send a command message to a particular service (with the routing key DoSomething). Some asynchronous stuff happens in the background, and an event message SomethingDone is broadcasted to all queues that are interested when something is done. This message contains the _id of a document that was affected, and then each service that heard the SomethingDone event requests via a request-reply queue for the document with the _id that was received in the event message (and whichever service knows about that document type sends a document message to the requesting service) (**).This is a very lightweight interaction method between services, and is especially useful when the communication can happen asynchronously. It may seem convoluted, but when you have your infrastructure set up, it can all happen pretty automatically.(*) There is a nuance between event messages that contain metadata, versus an event message that contains a document, which is described in the book(**) This is the “pull” method, as opposed to the “push” method. The “push” method would actually send the entire document along with the event message so that there doesn’t need to be a separate RPC request for the document after the event is received. For brevity, I’ll leave out the details here, and recommend you again to read the Message Types chapters in the bookMessage DeliveryQueues are especially useful when you have important messages that you want to know will be delivered. However, your design pattern for your messages and your message consumers should be idempotent (meaning, given the same message, the consumer will do the same thing each time). In other words, you don’t want a consumer that increments somebody’s bank account value by $1 every time it receives a message. Instead, you want the consumer to set the value to a specific value (which was $1 plus whatever the previous known value was). This is safer.There are caveats to this, and it depends on if, for example, your queues are durable, your messages are persistent, and your queue server is up (you can mitigate this risk too, by using an offline in-memory queue). However, given that your queue system itself is generally reliable, you have now managed to separate message delivery reliability from the application server itself.Places where this is helpful:  If an app server crashes and restarts  If you have to manually restart a server  If you need to scale a cloud server, which causes a restartFor points 1 and 2, most people will quip back with “well, you should have error handlers”. Yes, agreed. However, this assumes that the engineer managed to provide 100% error coverage, which isn’t realistic. Servers still crash, servers still need to be restarted, etc.For point 3, if you’ve used something like Heroku, you’ll notice that you have the option to scale horizontally, which means in Heroku’s context that you increase the number of dynos, which supports more traffic to the dyno. However, this also causes a server restart.Now, the benefit: I’ve seen a server restart, watched the logs, and then see a message get delivered (a message that was sent immediately before the server restarted). It didn’t really click in my head until I actually saw it happen in production in a critical instance: I restarted the server, and a message was still consumed by the process that I restarted.This is fantastic. However, it is also extremely important that your services consume messages in an idempotent manner. For example:A service sends a message to invoke a process that will take 15 seconds, and then the message will be acknowledged. However, the process crashed at 10 seconds, but unfortunately 10 database documents were already manipulated in that time. The message wasn’t acknowledged yet, so when the server boots up again, the message will be redelivered. So, will those updates cause the 10 documents to be updated again? Is there some value you’re incrementing, for example? That wouldn’t necessarily be idempotent.Queue Per ServiceIn HTTP, there exist common patterns for deciding how services will communicate (for example, REST). However, a pain point in building out a queue system from scratch is that you also have to figure out what “queue” means in your system. Do I set up a queue for certain message types (possibly)? Do I set up a queue per service and deliver all messages on that queue?The method I’ve had luck with in production so far is using a queue per service. This is the simplest way for me to reason about it in my head. You have a routing key for your message, and then it goes to a specific place (this would be for a “direct” queue using command messages).So, if you have a service that maintains user data, and the service is named “user-service”, then you create a queue called “user-service”. There are other patterns, but depending on your system, you may find the “queue per service” pattern useful.If you’re using RabbitMQ, you can just send a message with a specific routing key, and the queue server knows automatically which queue to send it to.Dead lettersThere are nuances between Dead Letter Channels and Invalid Message Channels. For brevity, again, I’ll recommend you read those 2 chapters in EIP, but for the sake of practical advice, I’ll tell you set up a dead letter queue.What do you do with those messages? Do you store them to a database? Do you redeliver them?Depending on your system, you could decrypt the message and store it in a database that provides encryption at rest. This will allow you a quick way to scan through the messages that couldn’t be delivered and debug (your mileage may vary here; depending on your system, you might have to encrypt certain fields yourself, etc.)EncryptionTL;DR  Use amqps:// protocol  Encrypt your message before sending to the queue serverIf you’re using the secure protocol, you have encryption while in-transit, but depending on your provider, you might not have encryption at rest.Encrypting your messages might be a hassle, since now your services all need to use, say, a shared key, but you can bury this under the hood of your API using a message adapater to automatically handle the key negotiation, encryption, and decryption.ConclusionAgain, I can’t recommend the EIP book enough. It can be hard to find coherent internet resources for implementing a queue, and this book provides well-tested production design patterns. Remember, consider your messages types, encrypt everything, and consider storing your dead letter messages to persistent storage to help you debug your system when (not if) something fails.",
      "url"      : "/rabbitmq/2017/08/14/practical-queue-considerations.html",
      "date"     : "2017-08-14 13:45:00 +0000"
    },
  
    {
      "title"    : "Understanding Mongoose Deep Population",
      "category" : "mongodb",
      "content"	 : "While MongoDB doesn’t natively support joins, the Mongoose framework now supports “deep population” (Mongoose has supported single-level populations for a while), which is akin to passing Mongoose a “graph” of what data should be populated in the results of your query. Who is this post for?  Those who have used or are familiar with Mongoose for MongoDB in a Node.js app and are familiar with basics of models, schemas, and queries.  You may also have noticed that to aggregate data, you have several built-in MongoDB and Mongoose options available, but realized that queries for aggregating data oftentimes result in having to make multiple round-trips between your server and the databaseIntroIt looks like Mongoose recently added sub-population to its API (see the exact line in the 4.1.0 (github) release, which is the first mention I can see of subPopulate).This means that if you upgrade from Mongoose 3.8.x to ^4.1.x (the latest release is 4.3.7 (github) at the time of writing), then you get this built in, as well as some other feature. Keep in mind, however, there are backwards breaking changes, but this guide (mongoose docs) covers how to migrate successfully.If you don’t know what sub-population is, or have never even heard of population, keep reading…Quick primer on populationSkip this section if you’re already familiar with Model.populate (mongoose docs).Let’s pretend we’re building a social app, and we have two models: a User and a Post:var UserSchema = {  _id: String,  username: String};var PostSchema = {  _id: String,  user: {    ref: &#39;User&#39;,    type: String  }};If you run this query: Post.find({}).populate(&#39;user&#39;).exec(callback), Mongoose will look at the field user in the post, see that it has a ref to the User model, and find that user by its _id (yes, right now, only _id (github issues), but this covers most general use-cases).In other words, this query might return you this:var results = [  {    __t: &#39;Post&#39;,    _id: &#39;1234&#39;,    user: {      __t: &#39;User&#39;,      _id: &#39;5678&#39;,      username: &#39;josh&#39;    }  }];It’s almost like a “join” in a SQL language, but not quite (remember, MongoDB doesn’t support joins, but there are some features like the Aggregation framework (MongoDB blog post), but I won’t cover that in this article). populate still requires some round-trips, but it optimizes this under the hood so that to you, the developer, it appears that you got all the aggregated data you requested in one fell swoop.Mongoose says this quite frankly:  There are no joins in MongoDB  Population is the process of automatically replacing the specified paths in the document with document(s) from other collection(s). We may populate a single document, multiple documents, plain object, multiple plain objects, or all objects returned from a query.If, for whatever reason, you didn’t find out about populate until you already have an app running in production and might not feel like changing schemas around or doing any migrations, you can also specify a model for your query (this means that your schema does not need a ref field):Post.find({}).popuplate({  path: &#39;user&#39;,  model: &#39;User&#39;}).exec(callback);If the query were to fail, your user field would just be null.So this is cool, but what if your User also has some field you want to populate? Let’s go deeper…Deep populationThis is the new thing I was talking about.Maybe our user has friends. Let’s update our User schema (we’ll drop the ref fields too):var UserSchema = {  _id: String,  username: String,  friends: [{ type: String }]};Now we can populate down an extra level:Post.find({}).populate({  path: &#39;user&#39;,  model: &#39;User&#39;,  populate: {    path: &#39;friends&#39;,    model: &#39;User&#39;  }}).exec(callback);Even better. Now we’ll get this as our result:var results = [  {    __t: &#39;Post&#39;,    _id: &#39;1234&#39;,    user: {      __t: &#39;User&#39;,      _id: &#39;5678&#39;,      username: &#39;josh&#39;,      friends: [        {          __t: &#39;User&#39;,          _id: &#39;9012&#39;,          username: &#39;barry&#39;,          friends: [&#39;3456&#39;, &#39;7890&#39;]        },        {          __t: &#39;User&#39;,          _id: &#39;3456&#39;,          username: &#39;rooney&#39;,          friends: [&#39;9012&#39;, &#39;5678&#39;]                }      ]    }  }];You might have noticed that josh’s friends also have friends. So you’ll have to define your graph even down further if you want to keep populating.So how does Mongoose fetch all the correct data?Under the hoodSo I talked about how Mongoose does some smart stuff behind the scenes. If you look at the actual query being made as a result of this populate query, it’ll look something like this under the hood:posts.find({});users.find({ _id: { $in: [&#39;5678&#39;, &#39;9012&#39;, &#39;3456&#39;] } });Aha! So it’s just an $in query! Mongoose collects all of the _id fields that it needs to look for per collection, and then after that… I’m not quite sure. Looking at the source code, it looks like it does some smart stuff to reflect on the results of that query and map the correct objects back to each original document, based on its position in the populate graph that you passed into the query… or something like that (you can look over the source code starting around line 2468 of lib/model.js if you’re so inclined).In other words, yes, we’re making multiple round-trips as expected, and you as the developer could do this same query yourself, but this nice Mongoose API simply let’s us define a graph of data that we want and takes care of the rest under the hood. Not sure how much of this might be similar to the structure of what GraphQL could be intending to do in the future (I would be interested to know, as I’m not very familiar with that).Let’s back up a second. We saw that we can start at one level and traverse down arbitrarily, but what about siblings?Sibling populationsLet’s back up and add something to our Post schema. We’re going to add “related posts”, which is just an array of Post IDs:var PostSchema = {  _id: String,  user: String,  related_posts: [{ type: String }]};So now we want to start at 2 different root nodes: the user field, and the related_posts field. Mongoose supports this too, and has for quite some time (but only deeply since recently):Post.find({}).populate([  {    path: &#39;user&#39;,    model: &#39;User&#39;,    populate: {      path: &#39;friends&#39;,      model: &#39;User&#39;    }  },  {    path: &#39;related_posts&#39;,    model: &#39;Post&#39;,    populate: {      path: &#39;user&#39;,      model: &#39;User&#39;,      populate: {        path: &#39;friends&#39;,        model: &#39;User&#39;      }    }  }]).exec(callback);As you can see above, we simply can define sibling graphs in an array. If you look starting at line 543 of the lib/utils.js source code, you can see that we check first if the graph is an array, and if so, map over each individual graph, and then looking for sub-populations, and recursing over and over until we collect all the relevant IDs, perform our $in query, and then map the resulting docs back to the original docs. Whew. I’m glad Mongoose does that for us.It seems like defining an arbitrarily deep graph every time we want to populate something might be a pain in the ass. Do we have to define graphs every time, or can we do it automatically?Automatic population per documentMongoDB published an article referencing Mongoose 4.0, and gave an example using the new pre and post find hooks (middleware):  But what if you always wanted to load the lead singer every time you loaded a band?// example from the article abovevar bandSchema = new mongoose.Schema({  name: String,  lead: { type: mongoose.Schema.Types.ObjectId, ref: &#39;person&#39; }});var autoPopulateLead = function(next) {  this.populate(&#39;lead&#39;);  next();};bandSchema.  pre(&#39;findOne&#39;, autoPopulateLead).  pre(&#39;find&#39;, autoPopulateLead);var Band = mongoose.model(&#39;band&#39;, bandSchema, &#39;bands&#39;);Nice. So if we define populations at the model level, we never have to actually call populate. So in our previous examples, we could just add the hooks to each model, and simply call this:Post.find({}, callback);Much cleaner. If you don’t want to handle this middleware yourself, you can use mongoose-autopopulate plugin:PostSchema.plugin(autopopulate);Some weaknesses and things that aren’t supportedIf you weren’t aware, you can have multiple models of different schemas all living in the same collection:mongoose.model(&#39;PhotoPost&#39;, PhotoPostSchema, &#39;posts&#39;);mongoose.model(&#39;TextPost&#39;, TextPostSchema, &#39;posts&#39;);Or, if you’re using mongoose-schema-extend:var PhotosPostSchema = PostSchema.extend({  // your schema}, { collection: &#39;posts&#39; });It’s hard for Mongoose to discriminate under the hood which IDs might be duplicated across documents, so you might end up with $in queries that have duplicate IDs if you’re querying the entire posts collection. Keep in mind, though, that you can also define different graphs by discriminatorKey (by default, __t). You do this with the match parameter:var populationGraph = {  match: { __t: &#39;PhotoPost&#39; },  path: &#39;user&#39;,  model: &#39;User&#39;,  // then continue with deep population};This can limit specific graphs to certain models which may have varying schemas that exist in the same collection.Some other things that aren’t supported by populate itself that I may write about in the future:  Renaming fields (for example, truly, user is just an ID… shouldn’t it be called user_id then in the database, but be returned to the client as user when it’s populated?)  Service calls (what if you want to populate a path based on some outside service?)ConclusionThis seems like a useful tool in being able to define a declarative model for what your data should look like as a result of any query. There are some inherent weaknesses to a lack of true “joins”, but the Mongoose API does an elegant job of optimizing these types of queries under the hood.I’ve only recently begun using this, so if you know something that I don’t and would like to contribute to the discussion for anyone reading this article, feel free to comment below with any critiques, suggestions, random quotes, or song lyrics. Thanks.",
      "url"      : "/mongodb/2016/01/24/mongoose-populate.html",
      "date"     : "2016-01-24 17:45:00 +0000"
    },
  
    {
      "title"    : "Breaking down HTTPS",
      "category" : "encryption",
      "content"	 : "What is HTTPS, and why is it any more secure than HTTP? What is TLS, RSA, symmetric and asymmetric encryption, and what happens when I send my credit card over a secure connection so that I can buy Game of Thrones with the sole intention of binge watching the entire season in one evening? This article will attempt to answer those questions (but not the one about your obsession with Game of Thrones).Who is this for?Intended audience: Anyone who knows what basic HTTP (not secure) is, and…  You want to know what makes HTTPS any more secure  You also want to know what terms like RSA, TLS, asymmetric, and symmetric actually meanYou’ll have a head start if you’ve ever purchased a domain certificate in the past from someone like Comodo or GoDaddy, or if you already know that HTTPS protects your communications over the internet and want to know how that actually works.Time to read: 20 minutes of uninterrupted reading.How did you get this information?Lots of reading and some software engineering. I obtained all this information from various sources on the internet (some of which may or may not be canonical), like various StackExchange websites, Robert Heaton’s blog, Schneier on Security, and Wikipedia. If you are generally skeptical of the veractiy of online sources which may or may not be opinionated, you’ll find some various specs and canonical references scattered throughout this article.If you want to see a step by step breakdown of an actual HTTPS request, I recommend The First Few Milliseconds of HTTPS by Moserware.IntroI recently read a great article by Robert Heaton (an engineer at Stripe), “How does HTTPS actually work?”. I would definitely recommend giving it a read.I wanted to extend off of some of the concepts a bit, and give some examples of how authenticity and encryption work on a very basic level within the context of communicating over the internet. I’ll do this by giving a simplified example of the HTTPS flow using asymmetric and symmetric encryption algorithms.For starters, I want to breakdown some terminology. Stay with me, we’ll clarify the whole HTTPS process again later:TLSRelated: What’s the difference between SSL, TLS, and HTTPS? on StackExchangeTLS (previously SSL) is the entire process used for secure communications over the internet. As you know, HTTP is the protocol used for clients and servers to exchange information (any type of information: sending HTML to the client, sending requests for, say, JSON to the server).When HTTP implements TLS, we call it HTTPS.TLS uses RSA for asymmetric cryptography (one part of what makes TLS secure). RSA in this case is only used for the initial connection to verify authenticity (the “handshake”, we call it). This is important! TLS also uses symmetric cryptography. This second form of cryptography is how messages (like sensitive login details or your credit card number) are actually encrypted and sent back and forth after the initial authenticity verification happens.Wait, wait, wait. Why don’t we just keep using RSA (our asymmetric cryptography) to send all of our messages back and forth? I’ll answer that soon. Keep reading.RSARelated: RSA (cryptosystem) on WikipediaWhat it isIt’s an algorithm. Specifically, it is the algorithm we choose to use for private and public keys. These two keys are different, and anything you encrypt with a public key can only be decrypted with the corresponding private key and vice-versa. We call this asymmetric cryptography. There are other algorithms besides RSA, but RSA is the most popular.It is important to note that calling a key “public” or “private” really only has to do with its availability to one person or multiple people.Another important thing to note is that when we “encrypt” with a private key, we always call this signing, because thousands of people may have the corresponding public key (which is the whole point). When this is the case, we can’t expect to actually “encrypt” a message with the private key, since anyone with the public key could “decrypt” that message: all we’re doing by signing is proving that it came from the person who has the private key (only one person has the private key). This is only verifying authenticity.However, when we encrypt something with the public key, we in fact do call this “encryption”. This is because only one person can decrypt the message: the person who has the private key.When we encrypt something with a private key, we always call it “signing”. When we encrypt something with a public key, we always call it “encrypting”.There is a comment I read to an answer on SHA1 VS RSA: what’s the difference between them?:  You shouldn’t talk about encryption with a private key when you mean signatures.Again, to really drive this point home:  Quick point of terminology: the public key isn’t used to decrypt a message encrypted with the private key: it’s used to verify (the signature of) a message that has been signed with the private key. Decrypting is done with the private key, following encryption with the private key. (It doesn’t make sense to encrypt something with the private key, so that anyone can decrypt it with the public key.)What it isn’tIt is not a hashing algorithm. Again, refer to the above SHA1 VS RSA: what’s the difference between them? on StackOverflow.The same applies for SHA2 (SHA-256), etc. For some reason, RSA and hashing algorithms get mixed up easily.RSA is the algorithm for creating public and private key-pairs. SHA2 (which is another name for SHA-256, by the way), for example, might be used to hash a password to store it in a database.ExampleHere’s a fake example of asymmetric cryptography:Private key:                            abcPublic key:                             xyzMessage we will &quot;encrypt&quot;:              hello worldOutput when signed by private key:      asdfghjOutput when verified by public key:     hello world(OK cool, since we could verify it, it came from the owner of the private key)Output when encrypted by public key:    zxcvbnmOutput when decrypted by private key:   hello world  OK cool, since we could verify it, it came from the owner of the private keyWait, what? If you didn’t know what the message was in the first place, how do you know it was really supposed to say hello world?As you might notice above, we have to have some sort of expectation of what the message should look like. We have to know that the message is supposed to be “hello world” in the first place. So what we do is send the original message along with a “digital signature”, which is simply the signed (remember, we don’t say encrypted when we encrypt something with a private key) version of the message. Then you just use the public key to verify (again remember, we don’t “decrypt” with a public key) the attached signature to make sure it matches the plain-text message.Here is a fake example of how that would look:var message = &#39;hello world&#39;;var signature = sign(sha256(message), privateKey); // =&amp;gt; a5dc78923jhbcvar certificate = message + signature;print(certificate); // =&amp;gt; hello worlda5dc78923jhbcWhen we send that message to someone, they parse out the digital signature (the a5dc78923jhbc part), and then verify and hash the original themselves to check if they are the same thing:verify(&#39;a5dc78923jhbc&#39;, publicKey) === sha256(&#39;hello world&#39;);In reality, certificates (such as the popular X.509 certificate) contain a different message, and the content of the digital signature is different. Here is the X.509 rfc, which echos the point above about hashing and then signing:  The data to be signed (e.g., the one-way hash function output value) is formatted for the signature algorithm to be used.  Then, a private key operation (e.g., RSA encryption) is performed to generate the signature value.There are some unanswered questions here, like, how do we know who this certificate actually came from. Could someone just copy and paste someone else’s certificate and send it (I don’t mean the private key; I mean the signature, which is public and attached to the certificate, which is also public)? More on this in a sec.Asymmetric Public / Private Key Encryption (RSA) in Node.js puts it very well:  Public keys Encrypt &amp;amp; Verify  Private keys Sign &amp;amp; DecryptSymmetric cryptographyRelated: Caesar cipher (Wikipedia)ExampleOn a high level, symmetric cryptography is something like this:Plain:    ABCDEFGHIJKLMNOPQRSTUVWXYZCipher:   XYZABCDEFGHIJKLMNOPQRSTUVWHELLO WORLD =&amp;gt; EBIIL TLOIAIf we both know the cypher, we can encrypt and decrypt messages.The client and server will decide which symmetric algorithm they want to use (thankfully, the Caesar Cipher is not one of the available options). A popular one is AES, and the one used before that was DES.To give an AES example in code (adapted from Encrypt and decrypt content with Nodejs):var crypto = require(&#39;crypto&#39;);var algorithm = &#39;aes-256-ctr&#39;;var password = crypto.randomBytes(16);function encrypt(text){  var cipher = crypto.createCipher(algorithm, password)  var encrypted = cipher.update(text,&#39;utf8&#39;,&#39;hex&#39;)  encrypted += cipher.final(&#39;hex&#39;);  return encrypted;} function decrypt(text){  var decipher = crypto.createDecipher(algorithm,password)  var dec = decipher.update(text,&#39;hex&#39;,&#39;utf8&#39;)  dec += decipher.final(&#39;utf8&#39;);  return dec;}In other words, we’re created a key: a bunch of cryptographically random bytes –  A cryptographically secure number random generator, as you might use for generating encryption keys, works by gathering entropy - that is, unpredictable input - from a source which other people can’t observe.Then we used AES-256 to use that key to encrypt our message. If we share that key, we can also decrypt that message.Message:           hello worldPassword:          2d00cd9bd332cef43a41d80ef31b9ebbEncrypted message: 940f1f7e63f7bc229169fa Symmetric algorithms are generally more efficient than their asymmetric counterparts, but we can’t send a session key (a symmetric shared key) over a non-secure network connection and expect any information encrypted with it to be secure. This is why we start off first with asymmetric encryption, and then end up using symmetric encryption for the rest of the time we’re connected. More on this in a sec.Revisiting the HTTPS flowKeeping it basic, here is essentially what happens when you visit a site that has https:// in the URL (I’m shortening this into 2 broad steps, but just know that there is more going on at a lower level):Step 1 of 2: Server proves its authenticity by sending a signed certificateRemember in the RSA section above where we asked, “how do we know who actually sent the certificate”? Even if there is a signature present, this only proves that the message was somehow signed by the server’s private key. But couldn’t anyone simply copy and paste a valid certificate and send it, pretending to be say, www.microsoft.com? Yes, but remember, the moment you encrypt something with Microsoft’s public key and send it back to the original sender, they will not be able to decrypt it (given that they don’t have the correct private key). Also, we rely on outside constraints like DNS servers to make sure the traffic is routed correctly.Another example of this is in real life: you could write a new employment agreement granting yourself double your current salary, create a stamp of your boss’s signature, then stamp the memo and send it to HR. It looks like it came from your boss, but HR needs to check to make sure it actually did.We can also sign certificates with as many signatures as we want, belonging to whomever we want.Remember, your browser comes pre-installed with “root certificates” that you automatically trust. So, if the server bought its certificate from GoDaddy, the certificate can be signed with, say, GoDaddy’s private key as well as the server’s private key, and you use those two corresponding public keys to check the certificate (in other words, you’re checking that the certificate came from the server since only the server should have its private key, and you also check that the server bought their certificate from someone you trust already, like GoDaddy – we call them the “Certificate Authority”). In other word, we can have any number of people sign the certificate. This is the Chain of trust (Wikipedia).As Heaton notes (in an example of Symantec hypothetical being the CA for Microsoft):  Symantec will have taken steps to ensure the organisation they are signing for really does own Microsoft.com, and so given that your client trusts Symantec, it can be sure that it really is talking to Microsoft Inc.To simplify things, here are some ways we can check that the certificate in fact was sent by the correct server (instead of an attacker just copy and pasting an entire certificate and sending it to you, adapted from a fantastic answer to “How are ssl certificates verified?” on StackOverflow):  The certificate is signed by a CA, any intermediate signatures, and the server’s private key.  Your browser uses the pre-installed certificates to verify any CA signatures in the server’s certificate.  The certificate contains an IP address or domain name, and the browser checks this is the server with which there is an open connection.  The client encrypts the symmetric key (more on this in a second) with the server’s public key; only the server can decrypt this.From Heaton’s article:  Note that the server is also allowed to require a certificate to prove the client’s identity, but this typically only happens in very sensitive applications.Step 2 of 2: The server and client start using symmetric encryptionThe client encrypts a “password” (the session key that will be used from now on to decrypt each others’ messages) and sends it to the server. Remember, the server can only decrypt this message containing the session key if the server actually has the corresponding private key, which is another cool security measure.So to answer a question from earlier, why don’t we just keep using RSA (our asymmetric cryptography) to send all of our messages back and forth? Essentially, asymmetric encryption is less efficient than symmetric encryption. Some of these reasons include:Why is asymmetric cryptography bad for huge data? on Crypto StackExchange:  Size of cryptogram: symmetric encryption does not increase the size of the cryptogram (asymptotically), but asymmetric encryption does […] it is safe to say that a symmetric scheme is orders of magnitude faster and less power hungry than an asymmetric one, at least for decryption (some asymmetric schemes, including RSA with low public exponent, are considerably faster on the encryption side than they are on the decryption side, and can approach the throughput of some symmetric cryptography).Someone mentioned succinctly from How does SSL use symmetric and asymmetric encryption? on StackOverflow:  Asymmetric encryption is necessary to verify the others identity and then symmetric encryption gets used because it’s faster.Remember too initially that a server can’t encrypt messages with the private key since anyone with the public key could read them. This would mean that the server would need to have the client’s public key as well so that the server could actually encrypt messages.From now on, since the client and server both have a shared password (the “session key”), they can both encrypt and decrypt messages with it.In our example from the “symmetric encryption” section (these are actual AES-encrypted keys and messages):Message:                      hello worldSession secret (password):    2d00cd9bd332cef43a41d80ef31b9ebbEncrypted message:            940f1f7e63f7bc229169fa Remember, since the client sent the server the session secret (by encrypting it asymmetrically first by using the server’s public key), the server and client both now used this shared session secret from now on.A note on trustPart of the Chain of Trust depends on us having pre-installed root certificates that we implicitly trust. That means I could make a certificate and install it on someone’s computer, and then pretend to be the server they are requesting something from. This is a classic “man in the middle” attack.You can use Charles Proxy to test your native app API. Essentially, your’re performing a MITM attack against your own app. To understand this, check out Reverse Engineering Native Apps by Intercepting Network Traffic (this was also linked at the end of Heaton’s article above, but I’d like to link to it as well since it is very relevant). You can also use Charles Proxy to perform a MITM attack on one of your co-workers. Remember there are laws against this (also see Hacker Law). Or your boss could monitor all of your internet traffic (there is probably a clause in your employment agreement allowing your IT department to perform MITM attacks on you). There’s even an article called 3 Tips for Legally and Ethically Monitoring Employees Online.Also, I mentioned above something about “buying a certificate”. If you haven’t purchased a certificate before in order to enable HTTPS on your custom domain, here’s how it works (I’ve bought 3 or 4 certificates from different providers in the past). Some providers (like Comodo) offer a free 90-day certificate where they don’t really verify much about you. On the other hand, you can buy a certificate from them, or from someone like GoDaddy, called “extended validation”, where they email the email address associated with the domain, they verify your business and address, etc. This allows the certificate authority to verify that they are giving a legitimate certificate to a legitimate entity. Does it always work out perfectly? I would say no.For example, I could still be a bad guy setting up an HTTPS domain to steal information from people, and I could simply get one of Comodo’s free 90-day certificates, steal a bunch of information from unsuspected users visiting my site over an HTTPS connection. In other words:  Certificate authorities sign a certificate simply saying it belongs to a domain (and that’s it)  The certificate allows you to use an HTTPS connection so people can’t steal data that’s going across the wireBut this doesn’t prove whether the owner of the server is good or bad. This whole process is just about identity, authentication, and secure communications. Just something to keep in mind.ConclusionIf you made it this far, I’d like to say… you have no life, and thank you for reading (just kidding about the “no life” part; you only spent 20 minutes reading this, I spent a whole day writing it… So who’s really the one with no life?)In pseudo-code using pseudo-certificates, here’s our summary of what happens over HTTPS:  Client connects and asks server for their certificate, which in my pseudo-world looks like this: ip:216.58.192.461:public_key:123456:signature:qa2ws3ed4rf  Client verifies that verify(&#39;qa2ws3ed4rf&#39;, public_key) === sha256(&#39;ip:216.58.192.461:public_key:123456&#39;)  Client creates a shared symmetric session secret caesarcipher, and encrypts this with the server’s public key  Client sends encrypt(&#39;caesarcipher&#39;, public_key) which turns out to look like abcdefg to the server  The server attempts to decrypt(&#39;abcdefg&#39;, private_key), and thankfully, the server gets caesarcipher (the correct shared session key)  The client buys the new season of Game of Thrones by sending his credit card number (encrypted with caesarcipher) to the server and binge watches the entire season in one eveningThat’s it. This is the real end. Again, this is a very, very basic overview and there is much more going on behind the scenes (for example, things like pre-master and master secrets, nonces, and all kinds of other stuff. Also, we are still discovering bugs in SSL).Further reading:  SSL/TLS &amp;amp; Perfect Forward Secrecy from Vincent Bernat  Are SSL encrypted requests vulnerable to Replay Attacks? on Security StackExchange (explains partially why we use randoms “so old signatures and temporary keys cannot be replayed.”)",
      "url"      : "/encryption/2016/01/17/breaking-down-https.html",
      "date"     : "2016-01-17 13:49:00 +0000"
    },
  
    {
      "title"    : "Using node.js to upload your app to Google Play",
      "category" : "javascript",
      "content"	 : "If you&#39;re developing a hybrid mobile app, you may want to stick with node.js to upload your app to the Google Play Store. However, Google&#39;s node.js client is still in &quot;alpha&quot;, and documentation is very limited. This post gives information about basic setup to get started using the googleapis client.Why would I use node.js to upload an Android app?Perhaps you’re developing a Cordova (hybrid mobile) app :)The only library I found available to upload to the Play Store is google-api-nodejs-client (written by Google).Since I couldn’t find much documentation, I had to comb a bit through the source code of the library to figure out what was going on. This post will go over some of the initial set-up of the library (there are a few holes, which are left to be figured out).This tutorial starts at the point after you’ve already set up your app in the Google Play store (but have not yet uploaded a new APK).Important: according to Play Store docs, you must first MANUALLY upload your APK the first time (by going into your developer console and clicking the “Upload” button). Any time after that, however, you can use a library such as the one in this tutorial.Also, I’m assuming you’ve already found out a way to build your APK. If you’re using Ionic/Cordiva, follow their official tutorial to figure out how to generate an APK.Useful documents before we start  My full example code used in this tutorial (on Github. Save it as something like upload.js, and remember to chmod a+x upload.js to be able to run it on the command line.) This may be incomplete or not fully functional in some places (it was originally tailored for one of our organization’s apps), but the basic “skeleton” is there. You can also download the ZIP (this is linked from Github).  googleapis README on Github  googleapis npm page  androidpublisher v2 source code, which is what I used as my documentation (it’s pleasantly well-commented)  Google documentation on developers.google.com for generic usage of the android-publisher moduleDownload the librarynpm install --save-dev googleapisThis is the abovementioned library, which, according to Github is:  Google’s officially supported node.js client library for using Google APIsSupposedly it supports all of Google’s APIs, including the androidpublisher (which is what we’ll be using).Uploading isn’t quite what it seemsWith this API, we don’t simply upload an APK.Instead, we:  Open an “edit” (play.edits.insert)  Upload the APK in sort of a limbo state (play.edits.apks.upload)  Do whatever else in this step (say, set a track, etc.)  “Commit” the edit (play.edits.commit)Basically what we’re doing is opening some sort of “container” in cyberspace where we have a white room to upload our app, set whatever properties, etc., and then finally we either discard or commit all those edits.Primer on “tracks”Read the official Google docs about tracks.“Tracks” are just channels (alpha, beta, and production). In the Play Store, you can put your app in any of these tracks. Why have alpha and beta? Well, in each one of those tracks, you can designate specific testers.For example, your alpha track can be closed testing for your developers only, and your beta track can be for, say, QA engineers, or a small subset of other people.These “limited” tracks (alpha and beta) are also fantastic if your app is some sort of “admin” app that will only ever be available to specific users in your organization. But be careful: the moment you send your app to production, it is there to stay (and it’s live to everyone in the Play Store) (see Unable to deactivate APK accidentally uploaded to Prod on StackOverflow).Just as an aside, if you want each APK in each track to hit a different server (for example, if you want your beta to hit “staging.example.com/api/1.0” and your production track to hit “production.example.com/api/1.0”), you’re out of luck (as far as I know). The APK you upload and promote through the various tracks can’t be reconfigured each time you move it to a different track (by the way, to move your beta to production, you can just hit the “Promote to…” button and click the new channel. I haven’t yet tried to use the API to do this, as opening a new zshell window is as easy as just logging into the Play Store console in Chrome and pressing the button. Some hardcore shell users may disagree with me.)Initial set upThere’s a lot going on here. Here are the initial module declarations.// our handy libraryvar google = require(&#39;googleapis&#39;);// this is optional, but helpfulvar Promise = require(&#39;bluebird&#39;);// just a utility library (handy, if you haven&#39;t used it before)var _ = require(&#39;lodash&#39;);// command line parsingvar argv = require(&#39;yargs&#39;).argv;// see below in &quot;Finding your secret.json&quot; to find out how to get thisvar key = require(&#39;../../../secret.json&#39;);// I&#39;m using my package.json as my source of truth for my versioningvar version = require(&#39;../../../package.json&#39;).version;// any unique id will do; a timestamp is easiestvar editId = &#39;&#39;+(new Date().getTime());// editing &quot;scope&quot; allowed for OAuth2var scopes = [  &#39;https://www.googleapis.com/auth/androidpublisher&#39;];// here, we&#39;ll initialize our clientvar OAuth2 = google.auth.OAuth2;var oauth2Client = new OAuth2();var jwtClient = new google.auth.JWT(key.client_email, null, key.private_key, scopes, null);var play = google.androidpublisher({  version: &#39;v2&#39;,  auth: oauth2Client,  params: {    // default options    // this is the package name for your initial app you&#39;ve already set up on the Play Store    packageName: &#39;com.example.app&#39;  }});google.options({ auth: oauth2Client });Finding your secret.jsonYou can make one! Go to https://console.developers.google.com.      Click on &quot;permissions&quot; in the side menu      You can generate your key here. Don&#39;t worry, as of the date of writing this post, you can generate as many keys as you&#39;d like without the fear of revoking other active keys! :)secret.json should look something like this:{  &quot;private_key_id&quot;: &quot;y34hr1ght&quot;,  &quot;private_key&quot;: &quot;-----BEGIN PRIVATE KEY-----nyouthoughtiwasgoingtopostmyprivatekeyn-----END PRIVATE KEY-----n&quot;,  &quot;client_email&quot;: &quot;someweirdlookingstring-123@developer.gserviceaccount.com&quot;,  &quot;client_id&quot;: &quot;someweirdlookingstring-123.apps.googleusercontent.com&quot;,  &quot;type&quot;: &quot;service_account&quot;}I found the initial options and how to initialize androidpublisher through a bit of trial and error. You can use the general googleapis README (which is not specific to androidpublisher) if you want to see example of connecting to other APIs, using OAuth2, etc. Again, you won’t find much useful information for this specific case.Let’s actually do some stuff// &quot;open&quot; our editstartEdit().then(function(data) {  var apk = require(&#39;fs&#39;).readFileSync(&#39;./Chronicled.apk&#39;);  // stage the upload (doesn&#39;t actually upload anything)  return upload({    edit: data.edit,    apk: apk  });}).then(function(data) {  // set our track  return setTrack(data);}).then(function(data) {  // commit our changes  return commitToPlayStore(data);}).then(function(data) {  // log our success!  console.log(&#39;Successful upload:&#39;, data);}).catch(function(err) {  console.log(err);  process.exit(0);});/** *  Sets our authorization token and begins an edit transaction. */function startEdit() {  return new Promise(function(resolve, reject) {    // get the tokens    jwtClient.authorize(function(err, tokens) {      if(err) {        console.log(err);        return;      }      // set the credentials from the tokens      oauth2Client.setCredentials(tokens);      play.edits.insert({        resource: {          id: editId,          // this edit will be valid for 10 minutes          expiryTimeSeconds: 600        }      }, function(err, edit) {        if(err || !edit) {          reject(err);        }        resolve({          edit: edit        });      });    });  });}/** *  Stages an upload of the APK (but doesn&#39;t actually upload anything) */function upload(data) {  var edit = data.edit;  var apk = data.apk;  return new Promise(function(resolve, reject) {    play.edits.apks.upload({      editId: edit.id,      media: {        mimeType: &#39;application/vnd.android.package-archive&#39;,        body: apk      }    }, function(err, res) {      if(err || !res) {        reject(err);      }      // pass any data we care about to the next function call      resolve(_.omit(_.extend(data, { uploadResults: res }), &#39;apk&#39;));    });  });}/** *  Sets our track (beta, production, etc.) */function setTrack(data) {  var edit = data.edit;  var track = tracks[argv[0] || &#39;alpha&#39;];  return new Promise(function(resolve, reject) {    play.edits.tracks.update({      editId: edit.id,      track: track,      resource: {        track: track,        versionCodes: [+data.uploadResults.versionCode]      }    }, function(err, res) {      if(err || !res) {        reject(err);      }      resolve(_.extend(data, { setTrackResults: res }));    });  });}/** *  Commits our edit transaction and makes our changes live. */function commitToPlayStore(data) {  return new Promise(function(resolve, reject) {    play.edits.commit({      editId: data.edit.id    }, function(err, res) {      if(err || !res) {        reject(err);      }      resolve(_.extend(data, { commitToPlayStoreResults: res }));    });  });}In the code above, all we’re doing is calling these steps in order:  Open our edit (startEdit)  Stage our APK for upload (upload)  Set our track (setTrack)  Committing our changes (commitToPlayStore)Throughout all these steps, all we’re doing is manipulating our data after each function call and piping it into the next function (nothing special about that part).If all went well, you should see something like this printed in your consol:Successful upload: { edit: { id: &#39;1234567&#39;, expiryTimeSeconds: &#39;1441766670&#39; },  uploadResults:   { versionCode: 50,     binary: { sha1: &#39;dddddd444444aaaaa55555&#39; } },  setTrackResults: { track: &#39;beta&#39;, versionCodes: [ 50 ] },  commitToPlayStoreResults: { id: &#39;1234567&#39;, expiryTimeSeconds: &#39;1441766670&#39; } }Forgetting to bump your versionIf you see this:{ [Error: APK specifies a version code that has already been used.]  code: 403,  errors:   [ { domain: &#39;androidpublisher&#39;,       reason: &#39;apkUpgradeVersionConflict&#39;,       message: &#39;APK specifies a version code that has already been used.&#39; } ] }[TypeError: Cannot read property &#39;edit&#39; of undefined]This means you need to bump your package.json version.Final thoughtsThe node.js client used in this tutorial is still very much in its early stages, as noted by Google in their official README:  This library is in Alpha. We will make an effort to support the library, but we reserve the right to make incompatible changes when necessary.Another gaping hole missing in this tutorial is promotion through tracks. If anyone knows more about that, please comment below! Thanks for reading, and I hope you’ve found some useful information in this post!",
      "url"      : "/javascript/2015/12/26/using-nodejs-to-upload-app-to-google-play.html",
      "date"     : "2015-12-26 15:30:00 +0000"
    },
  
    {
      "title"    : "Initial thoughts on event sourcing",
      "category" : "javascript",
      "content"	 : "&quot;The state of any object can be derived from the events that affect it.&quot; In a nutshell, this is the thesis of &quot;event sourcing&quot;, which, when implemented in an application, becomes an &quot;event driven architecture&quot;. In this post I&#39;ll discuss some of the practical applications, benefits, and drawbacks that the team and I have noticed after migrating our current, stateful database to a more event-driven approach.TL;DRStore events in your database, and construct the current state of your database from those events. Keep side-effects separate (like emails being sent to users). Endpoints become open-ended and asynchronous, and you can track the state of your database throughout time (almost like Git).IntroMy team and I are several months into a move from a traditional RESTful implementation (which is referred to as “active record” and is arguably the most widespread concept taught in web development) of our server to a more event-driven approach. If you haven’t read much about event-driven architecture (EDA) or event sourcing, I would recommend taking a look at this easy-to-follow slide deck (it should take about 10 minutes or so to get a basic idea of what’s going on). If you prefer not to read that…Here’s a quick rundown of some of the basic concepts  “Entities” (records in your database) are modeled as events (for example, UserCreated would be an entity).  Your stateful objects (something that would traditionally be a User object or whatever) are derived from your events.  In theory, if you only kept a backup of your event entities, you should be able to reconstruct the most up-to-date state of your entire database.Instead of mutating an object over and over and losing any notion of what led the object to that point, you can instead keep a timeline of the object and derive its current state. It’s like having a history of every object in your database. Kind of reminds me of Git.For performance reasons, every time some requests, say, user/1234, you don’t want to recompute that user object based on the events that led up to that point (UserCreated, UserUpdatedEmailAddress, UserLoggedIn, etc.), but instead, every time an update/delete occurs, you can just store the current state of whatever that object is in a separate table and query for that on any GET request.In other words, new events trigger updates, and GET requests query for the stateful object itself. This is almost like a cache.In other words, this is a very close model of real life. Things happen over time, and the current state of anything is just the sum of the things that happened to it.What can the endpoints look like?Something like /api/1.0/userCreated.One approach is to model your endpoints as events themselves. In other words, the endpoints have a semantic meaning for their corresponding entity, just like any other traditional RESTful endpoint. However, this approach lends itself to being a bit more open-ended and a bit less procedural. Additionally, all endpoints become basically either a GET or a POST (similar to “REST without PUT”), and side-effects are processed asynchronously on the backend.That last point about side-effects is important, because if you were to reconstruct your database from your events, you would want to do so without, say, triggering emails being sent to users, etc. The side-effects should be encapsulated in a set of logic seperate from the event creation itself (and endpoints are a great mechanism for this).Some benefits I’ve encounteredLoosely coupled, open-ended endpointsThis has been absolutely fantastic. For most operations, our client application just sends a GET for a current stateful object, or a POST to create an event. That’s it. If you’ve ever deployed an iOS application to the App Store, you’ll know it takes about a week (sometimes shorter, sometimes longer) to get approved. This means that if you have a bug in your client application, the fix takes the sum of the time it takes you to:  Receive a complaint from a customer  Find out why the bug is happening (“well, it was working on my machine?!”)  Fix the bug  Code review  Do some QA  And then, get App Store approvalThis means you need to remove flow of control from your client application as much as possible, and move that control to your server. This is pretty trivial in most cases with open ended endpoints that simply respond to events.We have continuous integration set up through CircleCI, which means that once we merge in Github from develop to staging, or staging to master, our server automatically deploys. This means that if we can keep as much logic as possible on our server, we can just deploy our server as often as we want to fix bugs that we encounter (this is obviously useless, however, if it’s a client UI bug).Backend becomes asynchronousThis is huge. This means that essentially all side effects happen inside workers. So the only response that gets sent back to the client is either something like a 401: Unauthorized, 500: Internal error, or 200: Success. These responses are only those of the event creation itself. Things that might take a shit ton of time (like making a third-party service send an email to user who just signed up) can take place in the background.An asynchronous backend can be implemented without an event-driven architecture (for example, by making traditional, RESTful, active record endpoints kick off async workers), but these approaches mesh very well together.Some drawbacks I’ve encounteredLots of entitiesEvents are entities, and, well… entities are entities too. In other words, if you store both events and the current state of your database, then you have a big database.Backend becomes asynchronousYou’ll notice the second point was mentioned as both a benefit and a drawback. If your endpoints operate asynchronously, this means that you can only rely on sending an HTTP response for the creation of the event itself. That’s it. No client logic can depend on any subsequent operations or side-effects that take place on the server, unless you implement some sort of two-way communication via polling, web sockets, etc..For example, say we want to prevent users from posting an ad for their used motorcycle on our app unless they’ve verified they’re email address. In a traditional, synchronous, active-record approach, we might do this:  POST /advertisement BODY: { type: &#39;motorcycle&#39;, make: &#39;Harley Davidson&#39; }  Server checks user.emailVerified === true?  If not, respond with, say, 401: Unauthorized  This response triggers logic on the client to show a prompt asking for the user to enter their email address in order to be sent an email with a link to verify itThis works great, if the backend is synchronous. What’s left to be discovered is the best approach for processing complex logic such as this with an asynchronous backend, which requires a different frame of thinking (something we’re still working on figuring out).Some final thoughts/questionsDeletionsDo you want “soft deletes” or “hard deletes”? In other words, do you want to actually remove a stateful record from your database, or simply mark it (via a flag) as “removed” or “deleted”, or whatever?DiffsSince this approach is already closely related to Git, one of the only major things missing is the ability to “diff” objects. These diffs could be stored inside the event itself:// UserUpdated eventUserUpdated = {  user_id: &#39;auth0|123456&#39;,  email: &#39;new_email@gmail.com&#39;,  updated: [    {type: &#39;MailingList&#39;, _id: &#39;aXd45&#39;, email: &#39;new_email@gmail.com&#39;},    {type: &#39;User&#39;, _id: &#39;bgg7x&#39;, email: &#39;new_email@gmail.com&#39;}  ]}There are still many questions to be answered, and lots of cool things that can be implemented here. It is a shift in the commonly-taught paradigm (the “active record” approach). I am excited to learn more from others who have experience with this! Thanks for reading, and feel free to chime in in the comments below.",
      "url"      : "/javascript/2015/12/24/initial-thoughts-on-event-sourcing.html",
      "date"     : "2015-12-24 15:26:00 +0000"
    },
  
    {
      "title"    : "Get rid of $scope, and extend into the view model",
      "category" : "angularjs",
      "content"	 : "AngularJS&#39;s controllerAs syntax is a good first step into being able to have some sort of sense of hierarchy in applications. However, large controllers can still get unwieldy. We can move towards controlling our controllers with angular.extend.TL;DRPut all your controller properties into an object literal, and extend that object into this:angular.extend(this, vm);After that, always use this to refer to controller properties (don’t use vm.whatever).A brief overview of controllerAsSkip to the next section if you already know how to use this.Basically, if you have a parent controller and a child controller nested within, you have to explicitly refer to $scope.$parent to access the parent controller from the child controller.However, with controllerAs syntax, we get a namespace.&amp;lt;div ng-controller=&quot;ParentCtrl as parent&quot;&amp;gt;    {{parent.something}}    &amp;lt;div ng-controller=&quot;ChildCtrl as child&quot;&amp;gt;        {{child.something}}    {{parent.something}}      &amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;But then in your controller, you might have to deal with this:// parent.controller.jsvar vm = this;vm.name = &#39;Bob&#39;;vm.job = &#39;Builder&#39;;vm.motto = &#39;Yes we can!&#39;;vm.speak = speak;function speak() {  return vm.motto;}Also, wondering why we’re using vm? Check out John Papa’s AngularJS Style Guide.Now imagine that, 1000x, when you have a controller full of lots and lots of stuff. Really, you could argue that you should consider leveraging directives and services for most of your business logic, but sometimes it’s difficult to do.By the way, controllerAs still knows about $scope. This fake “namespace” simply happens internally by attaching an object to $scope. So in the above example, our parent controller $scope would look like this:{  // a bunch of $$ angular properties, and then...  parent: {    name: &#39;Bob&#39;,    job: &#39;Builder&#39;,    motto: &#39;Yes we can!&#39;,    speak: function speak(){...}  }}In fact, if you were to inject $scope into that controller and ask for $scope.parent, you’d see all those properties. There’s nothing fancy about it.Extending the view modelEver heard of angular.extend? It’s pretty nifty. It basically just puts properties from one object into another object, without overwriting any properties. AngularJS already attempts to protect us from this by delimiting internal properties with $ or $$, so it’s not really a concern anyway, but it’s a nice added touch.So, we can just make our controller look like this:// parent.controller.jsvar vm = {  name: &#39;Bob&#39;,  job: &#39;Builder&#39;,  motto: &#39;Yes we can!&#39;,  speak: speak };angular.extend(this, vm);function speak() {  return this.motto;}However, you’ll notice that it’ll usually be better from then on to continue to refer to controller properties with this rather than vm, because any changes through data-binding will propogate only to this (our namespaced controller), and not to our vm object reference. You can leverage things like Function.prototype.bind if you get into hairy contexts (like forEach loops, etc.).",
      "url"      : "/angularjs/2015/08/23/extending-vm-into-this.html",
      "date"     : "2015-08-23 18:30:00 +0000"
    },
  
    {
      "title"    : "3 lessons in solving stupid engineering problems (without resorting to manslaughter)",
      "category" : "javascript",
      "content"	 : "Recently, some colleagues and I dreadfully spent (read: wasted) valuable man hours attempting to solve a bug in a hybrid mobile app. The cause of the bug? A missing &amp;lt;script&amp;gt; tag. Several important lessons can be learned from the ways in which we decided to go about solving this simple headbanger (no, not like you&#39;d do at a heavy metal concert; rather, where you actually consider banging your head on a solid object) of a problem.  Before we continue, here’s the problem and our solutionWe were attempting to integrate a certain OAuth library into a hybrid mobile app, which uses ionic (if you haven’t heard of it, it’s a pretty… interesting hybrid framework for building mobile apps with web technologies, aka, no Objective-C or Java).Basically, the OAuth library would just let us login to our app. Except we couldn’t login… Why? Because we were missing this line of code in our index.html:&amp;lt;script src=&quot;cordova.js&quot;&amp;gt;&amp;lt;/script&amp;gt;That took quite a while to figure out. Pretty simple solution, though, no?Lesson 1: Everyone is responsible for the environment  No, not that environment (well, that environment too, but that’s not what we’re referring to, here). We’re referring to the development environment.When you run ionic start myApp blank, it scaffolds a blank app for you. It also happens to set up an index.html for you, which already contains this cryptic line:&amp;lt;!-- this will be a 404 when in a development environment --&amp;gt;&amp;lt;script src=&quot;cordova.js&quot;&amp;gt;&amp;lt;/script&amp;gt;However, with plethora build tools available (like webpack), one can usually just require(&#39;ionic&#39;) or whatever. Instead, we got rid of the index.html and replaced it with our own, because we thought we were being super developers who could build an app in one step. There are many issues that play into this (for example, the whole point of a build tool such as webpack is to be able to be a super developer and build the app in one step… but we’ll ignore that for now).Anyway, the initial assumption was that the environment was set up correctly from the get-go. Usually this is a reasonable assumption, and everything did appear to be working correctly from the get-go, but there were some clues that we weren’t paying attention to.This isn’t anyone’s fault, per se, but the key takeaway is that when you run into a bug like this, you should do your due diligence and consider that the environment could have, in fact, been setup incorrectly. Had this been the first step, many man-hours would’ve been saved.We had three devs working on this problem, and all of us assumed that everyone else had made sure the environment was pristine. We never questioned it.Lesson 2: Everything is a clue  In our situation, we had two apps: the first app was already working, login and all. The environment, plugins, etc., in our second app appeared to be equivalent to those of the first app. But there was a nagging clue that was ignored.In the context of ionic, in order to make outside requests (i.e., XHR), you have to have the cordova-plugin-whitelist installed. However, it’ll give you a nice little warning if you specify that the app can access all outside origins (with an asterisk: *), and don’t supply a certain meta tag: No Content-Security-Policy meta tag found. Please add one when using the Cordova-plugin-whitelist plugin..Because our second app environment was theoretically equivalent to the first app, we should’ve expected to see this warning in our second app. However, even though this clue was extremely subtle, it could’ve led us to a solution much earlier. In fact, I admit that I noticed the warning wasn’t present, but discounted it as a non-clue.The key takeaway is that everything, no matter how subtle, can be a clue.Lesson 3: Question your assumptions… and then question them again  Our initial assumption was that it had to be the specific OAuth library we were using that was causing the problem.We spent several hours combing through the source code of the library, comparing the differences between execution in the first app and the second app. But they appeared identical. And that’s because they were identical. That’s because our problem had virtually nothing to do with the OAuth library.While delving into this source code did lead us to make some valuable discoveries (related and unrelated to the issue at hand), it was not the genesis of the bug. The key takeaway is that you should question your assumptions of where the root of the problem lies.How do you do this practically? Well, perhaps by verbalizing your assumptions, and making a list of them. Here’s what our list would’ve been:  Assumption 1: our OAuth library is causing the problem  Assumption 2: our plugins (whitelist, inAppBrowser, etc.) aren’t configured correctly  Assumption 3: our environment is set up correctlyThese assumptions led us to this potentially devastating logic:Assumption: our OAuth library sucksConclusion: therefore, we need to reimplement our login system  So how did we stumble upon a solution to this mystical problem?We drunkenly stumbled into our solution by attempting to implement a workaround to the inAppBrowser plugin. Wait, wot?Well, one of our other underlying assumpetions was that window.open wasn’t functioning properly (this is something the OAuth library was using interally). So we thought, why not try to use cordova.inAppBrowser.open instead, as suggested by some StackOverflow answers?This led us to this haunting error (I’ll probably have nightmares about it for years to come):Uncaught ReferenceError: cordova is not definedOnly at that point did we go back to the ionic docs and see that, in fact, you have to include this script tag in your index.html (even though we were attempting to use the magic of webpack):&amp;lt;!-- this will be a 404 when in a development environment --&amp;gt;&amp;lt;script src=&quot;cordova.js&quot;&amp;gt;&amp;lt;/script&amp;gt;  ",
      "url"      : "/javascript/2015/08/23/solving-stupid-problems.html",
      "date"     : "2015-08-23 13:39:00 +0000"
    },
  
    {
      "title"    : "3 reasons you should not be using Array.prototype.forEach",
      "category" : "javascript",
      "content"	 : "One of the main problems with forEach is that it primarily relies on side effects, whereas some native Array.prototype alternatives make use of semantically-correct programming paradigms (such as reduction, mapping, and filtering) and may in turn introduce less incidental complexity (and enhance readability) when writing code.  3) You should be filteringIn this example, we have an array, and we want to eliminate items from an array that don’t meet a specific criteria. Here’s how you’ll see it done with forEach:Bad  var filteredArray = [];[1, 2, 3, 4, 5].forEach(function(number) {  if(number &amp;gt; 3) {    filteredArray.push(number);  }});console.log(filteredArray);    [4, 5]  This is a typical implementation. With forEach, you simply push each object to a completely new array. You’ll notice that this introduces extra state to maintain. More state equals more brainpower needed to understand what is happening.Luckily, Array.prototype.filter already has you covered there. It simply returns a new array of the items we want.Good  var filtered = [1, 2, 3, 4, 5].filter(isBig);function isBig(number) {  return number &amp;gt; 3;}console.log(filtered);    [4, 5]  2) You should be mappingIn this example, we want to “change” each value in an array to something else. Here’s how we’d do it with forEach:Bad  var stringNames = [];var names = [{  first: &#39;Josh&#39;,  last: &#39;Beam&#39;},{  first: &#39;Ozzy&#39;,  last: &#39;Osbourne&#39;}]names.forEach(function(name) {  stringNames.push(name.first + &#39; &#39; + name.last);});    [&quot;Josh Beam&quot;, &quot;Ozzy Osbourne&quot;]  Again, naked looping requires us to create additional state within our application.Here, we’re gonna use map instead. Again, it simply returns a new array without requiring us to do it ourselves:Good  var names = [{  first: &#39;Josh&#39;,  last: &#39;Beam&#39;},{  first: &#39;Ozzy&#39;,  last: &#39;Osbourne&#39;}]names = names.map(fullName);function fullName(name) {  return name.first + &#39; &#39; + name.last;}console.log(names);    [&quot;Josh Beam&quot;, &quot;Ozzy Osbourne&quot;]  1) You should be reducingHere, we want to combine certain values in an array.Bad  var total = 0;[1, 2, 3, 4, 5].forEach(function(number) {  total += number;});console.log(total);    15  In other words, when you think “reduce”, think about “collapsing” items into a new item. Another way to think of it is to imagine you have an array filled with the words of a sentence, and you want to concatenate them all into one string.Good  var total = [1, 2, 3, 4, 5].reduce(addAll);function addAll(total, current) {  return total + current;}console.log(total);    15  ConclusionYou’ll notice one of the overarching concepts of all these three methods is that they all take the “functional route”, where they don’t necessarily require the manual creation of additional state. It is possible to introduce side effects within these methods, however, they don’t primarily rely on side effects to function. In other words, you’ll see that Array.prototype.forEach primarly relies on side effects. It never returns a value other than undefined unless you explicity force it to.Why am I referring to functional programming in the context of JavaScript, which is clearly not a functional programming language? Well, to quote the omniscient Wikipedia article on functional programming:  Eliminating side effects, i.e. changes in state that do not depend on the function inputs, can make it much easier to understand and predict the behavior of a programIn other words, it can take more “brain power” to understand what is happening in a forEach loop whose sole purpose is to mutate outside data based on side effects, whereas map, reduce, and filter all have immediate and clear semantic meanings to the programmer (that’s not to say, though, that you can’t use the power of those methods for evil).  ",
      "url"      : "/javascript/2015/08/15/3-reasons-you-should-not-be-using-foreach.html",
      "date"     : "2015-08-15 19:04:00 +0000"
    },
  
    {
      "title"    : "5 Things I Learned in My First Month at a Startup",
      "category" : "engineering",
      "content"	 : "In a departure from the normal content of this blog, I wanted to talk briefly about some of (what I think) are some important things I&#39;ve learned so far in my first month working for a Bay Area tech startup.In no particular order (and at a very high level of abstraction and with as little fluff as possible)…5) Don’t come with a problem. Come with a working prototype.A working prototype says much more about a proposed solution, than does talking about the proposed solution.4) Be both purist and pragmaticThis is a careful balance of business goals vs. coding nirvana, ruthless pragmatism vs. lofty purism. In other words, it’s both good to know exactly what the perfect solution should look like, and to know what compromises you have to make to have a working solution based on business timeline constraints.3) Conflict breeds results (but always listen)As our patient, level-headed CTO says, “One person scores, but the whole team gets them there.” By “conflict” I don’t mean the bad type of conflict. I mean the conflict that arises from a group of passionate people all trying to arrive at the best solution.The opposite side of the coin is: communication. “Today, I will learn nothing by speaking, but much by listening.” Beme engineer Matt Hackett (previously VP of Engineering at Tumblr) says: “Most important engineering skill: Humility.”2) The pieces eventually come togetherWhen working on teams of engineers (perhaps in an Agile environment), all the pieces of the product seem scattered, but eventually, everything will begin to cleanly merge back into the product that everyone has been imagining it would be.1) Work hardYou’re it. You directly influence the success or failure of your company. Dwayne Johnson says, “always be the hardest worker in the room.” This is a tough goal when everyone else is already working harder than everyone else in the room.  Great companies don’t hire skilled people and motivate them, they hire already motivated people and inspire them. People are either motivated or they are not. Unless you give motivated people something to believe in, something bigger than their job to work toward, they will motivate themselves to find a new job and you’ll be stuck with whoever’s left.— Simon Sinek, Start with Why: How Great Leaders Inspire Everyone to Take Action",
      "url"      : "/engineering/2015/08/11/5-things-i-learned-in-my-first-month-at-a-startup.html",
      "date"     : "2015-08-11 17:48:00 +0000"
    },
  
    {
      "title"    : "Modern Looking CSS Toggle Switch",
      "category" : "javascript sass",
      "content"	 : "Create a nice, modern toggle switch that works by just changing its padding and adding a transition (all it takes is a JavaScript click handler to add or remove a class)!JS BinWorks in the latest version of IE, Chrome, Firefox, and Safari.This toggle switch can be used to turn things into an “on” or “off” state (for example, through HTTP requests via AJAX), and the state is represented by the toggle’s CSS.ComponentsThe toggle is made up of two components:The “toggle” (the container for the whole thing):This toggle gets the badass class name of, you guessed it, .toggle. It can also receive the .on class via JavaScript.The next component is the “switch” (the little round thing that moves back and forth):&amp;lt;div class=&quot;toggle&quot;&amp;gt;  &amp;lt;div class=&quot;switch&quot;&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;Principle of functionThe only thing that changes when you click the toggle is its padding!When you click on the toggle, it gets an “on” class added or removed, through some nice vanilla JavaScript:[].forEach.call(document.getElementsByClassName(&#39;toggle&#39;), function($toggle) {  $toggle.addEventListener(&#39;mouseup&#39;, function() {    this.classList.toggle(&#39;on&#39;);  });});What this “on” class does is set the padding-left of the toggle () so that the switch appears to move to the side.All we do after that is add some nice CSS3 transitions to the background-color and padding by saying something like transition: 400ms cubic-bezier(0, 0, 0, 1);, and we have a working switch.Don’t SASS me.toggle {  // this is both the width and height of the little circular switch  $switch-height: 36px;    // change the spacing between the switch and the entire toggle  $switch-margin: 2px;    // change the width of the whole toggle  $toggle-width: 70px;    // don&#39;t change these calculations  $toggle-height: 3 * $switch-margin + $switch-height;  $toggle-padding: $toggle-width - $toggle-height;    background-color: #eee;  border: 1px solid #fff;  border-radius: $toggle-height;  box-shadow: 0 0 5px #ddd;  box-sizing: border-box;  height: $toggle-height;  transition: 400ms cubic-bezier(0, 0, 0, 1);  width: $toggle-width;    &amp;amp;.on {    background: lightblue;    padding-left: $toggle-padding;  }    .switch {    background: #fff;    border-radius: 100%;    height: $switch-height;    margin: 2px;    position: relative;    width: $switch-height;    z-index: 9;  } }In its simplest formFor the slow people like me, here is a bare-bones version using regular ol’ onclick and CSS (and some bad practices).JS BinHTML:&amp;lt;div class=&quot;toggle&quot;&amp;gt;  &amp;lt;div class=&quot;switch&quot;&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;CSS:.toggle {  background: #eee;  box-sizing: border-box;  transition: 500ms;  width: 150px;}.toggle.on {  padding-left: 100px;}.switch {  background: white;  border: 1px solid gray;  height: 50px;  width: 50px;}JavaScript:document.getElementsByClassName(&#39;toggle&#39;)[0].onclick = function() {  this.classList.toggle(&#39;on&#39;);}ConclusionWell that’s that: a simple CSS selector switch (all it needs is a click handler). You could also make this an AngularJS directive called, say, &amp;lt;toggle&amp;gt;&amp;lt;/toggle&amp;gt;, and add the click handler inside the directive.Key points: The switch moves by just changing the padding, and you can add a transition to make it all smooth-like.",
      "url"      : "/javascript/sass/2015/05/26/modern-looking-css-toggle-switch.html",
      "date"     : "2015-05-26 19:07:00 +0000"
    },
  
    {
      "title"    : "From JavaScript to Ruby: Style Guide",
      "category" : "javascript ruby",
      "content"	 : "This post is the second in the series of “From JavaScript to Ruby”, which is aimed at helping JavaScript developers transition their thinking from JavaScript to Ruby. Here you’ll find a table of the ways we do things in JavaScript, and the way you’re supposed to do them in Ruby.Style GuideThis post is a living document. Expect changes as necessary. Suggestions? Leave a comment below or email Josh.Last updated: Apr 20, 2015                    If you do this in JavaScript...                Airbnb                    ...do it like this in Ruby                bbatsov                        String quotes              &#39;Some string&#39;      &#39;Some string&#39;              String concatenation              var world = &#39;world!&#39;,    helloWorld = &#39;hello &#39; + world;            world = &#39;world&#39;hello_world = &quot;hello #{world}&quot;                    Variable names                      camelCaseVariable = true                    camel_case_variable = false                    Tabs              function foo() {    // &#39;hard tab&#39;, 4 spaces}            def foo  # &#39;soft tab&#39;, 2 spacesend                    Callbacks              function someFunction(cb) {  var hello = &#39;hello&#39;;  if(typeof cb !== &#39;undefined&#39;) {    return cb.call(this, hello);  }}someFunction(function (h) {  return h;});            def someMethod  hello = &#39;hello&#39;  yield(hello) if block_given?endsomeMethod do |h|  hend# orsomeMethod { |h| h }                    Talking about methods              someClass.someMethod      SomeClass#some_method              Comments                      // single-line comment/*  multi-line  comment*/                    # single-line comment# multi-line# comment                ",
      "url"      : "/javascript/ruby/2015/04/20/from-javascript-to-ruby-style-guide.html",
      "date"     : "2015-04-20 14:57:00 +0000"
    },
  
    {
      "title"    : "From JavaScript to Ruby: Using Blocks Like Callbacks",
      "category" : "javascript ruby",
      "content"	 : "This post is the first in the series of “From JavaScript to Ruby”, which is aimed at helping JavaScript developers transition their thinking from JavaScript to Ruby. This post will answer this fundamental question: how do I do “callbacks” in Ruby? The answer is: the “idiomatic” way (we’ll talk about what “idiomatic” means) to use so-called callbacks in Ruby is to use blocks. We’ll discuss blocks, and their similarities to JavaScript callbacks.Defining “callback” through practical examplesA callback is a function that happens after we call another function, but the catch is, they’re coupled. However, being coupled in this way doesn’t mean we can’t reuse the callback function.We often use anonymous functions in JavaScript as callbacks. Here’s an example using Array.prototype.forEach:var names = [&#39;Bob&#39;, &#39;Sue&#39;, &#39;Aron&#39;, &#39;Joseph&#39;];names.forEach(function(name) {  console.log(&#39;Hello &#39; + name);});// =&amp;gt; &quot;Hello Bob&quot;// =&amp;gt; &quot;Hello Sue&quot;// =&amp;gt; &quot;Hello Aron&quot;// =&amp;gt; &quot;Hello Joseph&quot;We can actually sort of “scoop out” the function that we passed into Array.prototype.forEach in order to make it reusable:var names = [&#39;Bob&#39;, &#39;Sue&#39;, &#39;Aron&#39;, &#39;Joseph&#39;];names.forEach(sayHello);function sayHello(name) {  console.log(&#39;Hello &#39; + name);}// =&amp;gt; &quot;Hello Bob&quot;// =&amp;gt; &quot;Hello Sue&quot;// =&amp;gt; &quot;Hello Aron&quot;// =&amp;gt; &quot;Hello Joseph&quot;How do we do this in Ruby? And what does “idiomatic” mean?You’ll hear a lot of Rubyists use the word “idiomatic”. Read:  The idiomatic way to pass arguments to a method in Ruby is such and such…  The idiomatic way to use a “callback” in Ruby is to such and such…In normal-people speak, you’d say:  The common way to go about doing this is such and such…Make sense? In other words, there are certain patterns and best practices (solutions) for common problems in Ruby: we call these solutions “idioms”. So, the idiomatic way to use callbacks in Ruby is not to use callbacks at all. Instead, we use blocks.Here’s how we can do the same exact thing in Ruby, instead of JavaScript:If you run the above, you’ll see Hello &amp;lt;name&amp;gt; printed for each name, and at the end it’ll actually return the entire array for you to use.Let’s compare the two. Essentially, this JavaScript…:names.forEach(function(name) {  console.log(&#39;Hello &#39; + name);});…is the same as this Ruby:names.each do |name|    puts &quot;Hello #{name}&quot; endWe can also write the above Ruby in another, shorter way:names.each { |name| puts &quot;Hello #{name}&quot; }So here’s a few things we’ve learned from the above.1st thing we learned (string interpolation)# Notice the necessary double-quotes. Interpolation doesn&#39;t work without them.puts &quot;Hello #{name}&quot; # That&#39;s &quot;string interpolation&quot;. It&#39;s the same as this:puts &#39;Hello &#39; + nameBoth ways are correct, but the string interpolation notation is more idiomatic.In JavaScript, we call it “string concatenation”:console.log(&#39;Hello &#39; + name);2nd thing we learned (block syntax)We also learned two styles, or syntaxes, for writing blocks:names.each do |name|  # stuff hereend# do ... end is one type of block notation. The other type is:names.each { |name| }# The curly braces replace the &quot;do&quot; and &quot;end&quot;.  Pro Tip      Before we continue, I want to point out a couple style points. Rubyists often use &quot;soft tabs&quot;, or 2 spaces instead of just a 4-character tab, only because it different environments it remains readable (see this StackOVerflow question). Also, look at the spacing between the curly braces and pipes. I recommend something to you: Rubocop, which is sort of the de-facto command line tool that tells you if your code looks bad according to the community style guide on GitHub. It&#39;s good stuff.  3rd thing we learned (Enumerable#each == Array.prototype.forEach)Continuing, we learned that Ruby’s Enumerable#each is equivalent to JavaScript’s Array.prototype.forEach (more or less). I have found only one online blog post so far that has made Ruby’s #each make sense to me. It is by Erik Trautman.“Scooping out” the block like a reusable callbackThere are two common ways to make a reusable “callback” in Ruby. They are the lambda and the Proc. There are very subtle differences between the two ways.LambdaBy the way, that’s the new syntax for the single-line lambda as of Ruby 1.9 (I have Ruby 2.1 right now, if you’re curious). If you want to do a multi-line lambda, you do:sayHello = lambda do |name|            puts &quot;Hello #{name}&quot;           endNow that’s some weird-ass looking syntax, huh? Let’s make it look even weirder:Proc (short for “procedure”)Did you notice, by the way, that you can do multi-line variable assignment? Pretty cool; you can’t do that in JavaScript.Differences  Lambda:          You don’t have to pass in all the arguments if you don’t want to. If your “callback” needs, say, 3 arguments, and you only pass in 2 when you use it, there will be no error. This is just like a JavaScript callback… but in JavaScript, the undefined arguments will be undefined, and in Ruby, the undefined arguments will be nil.      If you call return &amp;lt;whatever&amp;gt;, the lambda stops, but any method that the lambda is in does not stop. You can use the lambda’s returned value in the rest of the containing method.        Proc:          You must pass in all the arguments, or you’ll get an error.      If you call return &amp;lt;whatever&amp;gt;, the Proc and any method that it is in will stop. The Proc will return its value to the containing method, and the containing method will also return that same value.      lambda in JavaScript:// This is a JavaScript lambdafunction world() {  return &#39;world!&#39;;}// This is the &quot;containing method&quot; that uses the lambdafunction hello() {  // world() returns, but it just gives up its value, and the function continues  var who = world();  return &#39;Hello &#39; + who;}// =&amp;gt; &quot;Hello world&quot;Proc in JavaScript:// This is a JavaScript Procfunction world() {  return &#39;world!&#39;;}// This is the &quot;containing method&quot; that uses the Procfunction hello() {  // The containing function returns the return value of the Proc  return &#39;Hello &#39; + world();  // Any code below here obviously won&#39;t run:  console.log(&#39;nothing will log here&#39;);}// =&amp;gt; &quot;Hello world&quot;In Ruby, though, as a side note, we rarely use an explicit return statement. You can just type a value.This JavaScript…:function hello() {  return &#39;Hello&#39;;}…is equivalent to this Ruby:def hello  &#39;Hello&#39;endThe ampersand (&amp;amp;)This confusing thing basically just makes sure the thing you’re passing into the method is a Proc, sort of. You can read about that in detail on a.blog.about.code.names.each(sayHello) # =&amp;gt; error: wrong number of arguments (1 for 0)The above error is basically saying, “why did you pass me an argument? I expected 0 arguments for #each”. &amp;amp;sayHello is not an argument! It is a Proc, and Ruby does not count Procs as arguments!It’s weird, huh? That’s because you can pass in a block at the end of pretty much any method in Ruby, and it won’t see it as an argument, in the typical sense.Check it out:# =&amp;gt; What&#39;s up# =&amp;gt; What&#39;s upWe passed in a block at the end, but it didn’t run. But we didn’t get any error either. If we want the block to actually run, we can just say yield:# =&amp;gt; What&#39;s up# =&amp;gt; What&#39;s up# =&amp;gt;  you#block_given? is a native method that returns a boolean (true or false) that checks if a block was passed in to the method. We have to use it in the above example, because if we just say yield instead of yield if block_given?, then we’ll get an error that says no block given (yield).Also, the question mark ? is Ruby’s way of meaningfully letting the user know that the method is supposed to return a boolean. There’s nothing really special about it.Now, going back to the original error (error: wrong number of arguments (1 for 0)):I’ll try my best to sort of construe a similar error in JavaScript:names.forEach(names, sayHello); // TypeError: [object Array] is not a function// It&#39;s saying &quot;names&quot; is not a function, which is true...// forEach expects one argument (a function)// we expected: names.forEach(sayHello);If the above code were Ruby, you’d get an error something like wrong number of arguments (2 for 1). Once again, I want to emphasize this is because Ruby doesn’t see the Proc as an argument.Conclusion  Use blocks as if they were callbacks.  Use a lambda or a  Proc. Remember the two subtle differences between them (number of arguments, and return values).  Put a &amp;amp; before the block that you pass into the method, if you pass it in to a native Ruby method that will run a block, like Enumberable#each.Further reading  Reactive.IO: Understanding Ruby Blocks, Procs, and Lambdas  makandracards: Short Lambda Syntax in Ruby 1.9",
      "url"      : "/javascript/ruby/2015/04/20/from-javascript-to-ruby-using-blocks-like-callbacks.html",
      "date"     : "2015-04-20 11:40:00 +0000"
    },
  
    {
      "title"    : "Something no one tells you about minifying AngularJS controllers (until it&#39;s too late)",
      "category" : "javascript angularjs",
      "content"	 : "This post is going to talk briefly about the common ways people are shown to write AngularJS controllers, why minifying your code will break your application if you write them in this way, and how to fix this problem.The usual approach to writing controllersIn many online AngularJS tutorials, you’re taught (for simplicity’s sake) to write a controller as such:var app = angular.module(&#39;myModule&#39;);app.controller(&#39;MyController&#39;,function($scope) {$scope.doSomething = function() {// some code here}});The above code has several advantages. First, it’s simple to read. Second, it’s simple to write. However, it is generally recommended to minify your JavaScript code before deploying it live, since this reduces the size of the file that the server has to send (in other words, this can dramatically increase the user’s perception of how fast your application loads).Minifying the above controller will break your application. Here’s why…Quick little tangent: if you’re curious, I use Gulp to concatenate and minify my project files. If you’re just writing some code in JSBin, for example, you can just head over to an online minifier like jscompress, which will do the job just fine in most cases.Continuing on… if you minify our above example and open your app, you’ll see that nothing happens. And you’ll get this error in your console:Error: [$injector:unpr]So what is this error? If we click on the error in our console, it will take us to the AngularJS website, which will say something like this:  This error results from the $injector being unable to resolve a required dependency. To fix this, make sure the dependency is defined and spelled correctly.In other words, because of the way AngularJS uses dependency injection (it actually parses the arguments of your functions), when the code is minified, $scope is no longer $scope, $route is no longer $route, etc. Instead, they’re just minified variables, like e or a, which don’t make any sense to AngularJS (if you want to understand a little bit more about dependency injection and how it works in JavaScript, check out these two posts: one by Anand and one by Alex. For now, just know that minfiying breaks dependency injection.)In fact, I’ve written two JSBin examples for you to see what exactly is going on.Working exampleJS BinNon-working (minified) exampleJS BinOpen up your console (if you’re on Mac with Chrome, hit CMD+opt+j), and you’ll actually see the injector error live on this site (since I embedded the JSBin).How to fix itWhen you click on your error message in the console, which leads to the AngularJS website, they actually provide you with a solution to this error:angular.module(&#39;myApp&#39;, []).controller(&#39;MyController&#39;, [&#39;myService&#39;, function (myService) {  // Do something with myService}]);This way works completely fine. If you minify, AngularJS instead looks at each item in the array (which is the second argument passed to the controller in the above example), and resolves any minified dependency to match its correct string name.But as you may or may not know, I am a big fan of John Papa’s Style Guide, and he specifically recommends against the above example, for several reasons. However, the biggest reason to me, is that with a long list of dependencies, the above code can get very hard to read, very quickly.Just imagine if we have something like this:angular.module(&#39;myApp&#39;, []).controller(&#39;MyController&#39;, [&#39;$scope&#39;, &#39;$route&#39;, &#39;Item&#39;, &#39;items&#39;, &#39;utils&#39;, &#39;shade&#39;, function ($scope,$route,Item,items,utils,shade) {  // Do something}]);Okay, okay, it’s not terrible. But I think there’s a better way to write it. As John Papa says:  Why?: Avoid creating in-line dependencies as long lists can be difficult to read in the array. Also it can be confusing that the array is a series of strings while the last item is the component’s function.Use $inject:angular.module(&#39;myApp&#39;, []);MyController.$inject = [&#39;$scope&#39;, &#39;$route&#39;, &#39;Item&#39;, &#39;items&#39;, &#39;utils&#39;, &#39;shade&#39;];angular.module(&#39;myApp&#39;).controller(&#39;MyController&#39;, MyController);function MyController($scope,$route,Item,items,utils,shade) {// do something}The above makes me breathe a sigh of relief, due to the now ease of readability. And, we’ve solved our issue of minification. We are now minification-safe! Anyone looking at the code can now understand exactly what is happening. “Oh, okay, so we have injected several services into this controller…”You’ll notice too that I separated the declarations of the module (I didn’t chain .controller to angular.module(&#39;myApp&#39;,[])). Once again, I recommend reading John Papa’s style guide for quick clarification on that.ConclusionThis doesn’t just go for controllers. It goes for anything where you inject anything. Directives, configs, runs, factories, etc. I am of the opinion that following the above implementation of dependency injection, at least in the current version of AngularJS at the time of writing this (end of March 2015), is probably one of the best solutions to solving the minification and readibility issues caused by other solutions.So to recap what we learned… Minifying code that uses string-dependent dependency injection can break your application, unless you explicitly define the strings to use for the dependencies, which $inject is perfect for.",
      "url"      : "/javascript/angularjs/2015/03/31/something-no-one-tells-you-about-minifying-angularjs-controllers-until-its-too-late.html",
      "date"     : "2015-03-31 10:48:00 +0000"
    },
  
    {
      "title"    : "Getting started with a search engine for your site (no server required!)",
      "category" : "javascript jekyll tutorial",
      "content"	 : "Today we&#39;ll talk about how to build a JavaScript-only search engine (using JSON and AJAX) to avoid databases and server-side programming. We&#39;ll do it all specifically in the context of Jekyll and GitHub Pages!What do you mean “no server”?Lots of search features on websites rely on communicating with the server to deliver search results. For example, a user might click a search button that sends, say, a POST request to the server, where a .php file handles the request, and sends the results back.With the way we’re gonna do it here, we’re going to handle the request entirely on the client. No server-side code processing required. We’ll go over the architecture in a minute.Why not process on the server?Nothing wrong with the traditional way of doing it. For my website, though, I’m using Jekyll, and I’m hosting it on GitHub pages. GitHub pages doesn’t support processing with PHP, Node.js, etc. Therefore, the only way to do it is on the client (with a little bit of pre-processing, which we’ll go over in a second).TutorialTechnologies used  Jekyll (a static site generator)  lunr.js (a JavaScript search indexer)  jQuery (to make the AJAX stuff and displaying the results easier)Architecture  Take advantage of Liquid (the template system that Jekyll uses) to create a JSON file of all our searchable content (in this example, blog posts)  Write some JavaScript that sends an AJAX request to retrieve the JSON file whenever the user searches for something  Use lunr.js to match the search query against all the blog posts in the JSON file and display the search results in order by the strength of the match&amp;hellip;Step 1 — Make the JSON fileWe’re going to kind of “hack” our way through Liquid to create a JSON file.Create a new file in your root called posts.json. Open it up:------[  {% for post in site.posts %}    {      &quot;title&quot;    : &quot;{{ post.title | escape }}&quot;,      &quot;category&quot; : &quot;{{ post.categories | join: &#39; &#39; }}&quot;,      &quot;content&quot; : &quot;{{post.content | strip_html | strip_newlines | remove:  &quot;&quot; | escape | remove: &quot;&quot;}}&quot;,      &quot;url&quot;      : &quot;{{ site.baseurl }}{{ post.url }}&quot;,      &quot;date&quot;     : &quot;{{ post.date }}&quot;    }{% unless forloop.last %},{% endunless %}  {% endfor %}]You’ll notice some strange things. First, if you’re using syntax highlighting, you’ll get all kinds of weird “errors”. Ignore them. JSON or JavaScript syntax highlighting doesn’t understand that we’re using Liquid.Second, how the hell is this going to work, if it’s a JSON file? Well, you see the two sets of --- at the top of the file? When you run jekyll build, it will see this file as a “special file” that needs to be processed with Liquid. If we remove the ---, it won’t process the template. This is called “YAML front matter”. Any pages with YAML front matter get processed with Liquid.Most of the templating above is self-explanatory if you understand the basics of Liquid filters. However there is one line I’d like to explain.&quot;content&quot;   : &quot;{{post.content | strip_html | strip_newlines | remove:  &quot;&quot; | escape | remove: &quot;&quot;}}&quot;,The content of our post might contain raw tab characters, as well as double quotes (&quot;). Well, as it turns out, having tab characters inside a JSON string is invalid JSON, so when we call our AJAX request later, nothing would be returned! Not good.Solution? Run two remove filters: one for the tab character, and one for the double quotes.Also, I want to explain this part:}{% unless forloop.last %},{% endunless %}If you don’t have that line of code, your output would look something like this:[{ /* content */ },{ /* content */ },{ /* content */ },]See the trailing comma on the last object? This is also invalid JSON. Not good. So, we run a some Liquid that says don’t put a comma at the end if it’s the last object.Now, run jekyll build, and you’ll end up with a compiled posts.json underneath _site, which contains the entire built site. Here’s the compiled JSON:[    {      &quot;title&quot;    : &quot;Why do people add semicolons before modules?&quot;,      &quot;category&quot; : &quot;javascript snippet&quot;,      &quot;content&quot; : &quot;Simple answer: because of minification issues.Minification can cause modules to use each other as their arguments (unintentionally), if the developer isn’t careful.(function() {// code})()(function() {// code})();If you look closely enough at the above, you’ll see the first IIFE is missing a semicolon at the end.That means that when minified, it’ll look like this:The broken code:(function() {})()(function() {})();// (a)()(b)()The problem is that now function a is being called with function b passed in as an argument. Interesting.So, we just add a ; to the beginning of the module, and to the end. This acts as a safeguard to ensure we don’t run into that problem when we minify.So, when we try doing the above example with semicolons at the beginning and at the end, and you minify the code, you get this instead:The working code:;(function() {})();;(function() {})();The cool thing is, the above doesn’t throw any errors. In fact, JSHint won’t yell at you either.Why is this? Check out the MDN article on empty:  An empty statement is used to provide no statement, although the JavaScript syntax would expect one.So those extra semicolons are not a syntax error, because a random semicolon anywhere in the code can be interpreted as an empty statement. Cool, huh?&quot;,      &quot;url&quot;      : &quot;/javascript/snippet/2015/03/22/why-do-people-add-semicolons-before-modules.html&quot;,      &quot;date&quot;     : &quot;2015-03-22 18:34:00 -0500&quot;    },    /* and several other posts*/]I strongly recommend running your output through JSONLint if your code isn&#39;t working right. It&#39;ll help you find whatever errors you might have in your JSON syntax.Step 2 — Send an AJAX requestCreate a “Query” objectDon’t confuse our Query object with jQuery. Our Query object will serve as a container for everything related to our search. I’ve commented the code so you can see what everything does.//Create a module using an IIFE;(function(global,$) {  /*    Put ourselves into &quot;strict&quot; mode    This just helps us write cleaner JavaScript  */  &#39;use strict&#39;;  Query.prototype = {    // this.q is our search query (for example, &quot;javascript tutorial&quot;)    set: function(val) {      this.q = val;      return this;    },    // brings us to our search page with a query string attached    goToLocation: function(route) {      if(typeof this.q !== &#39;undefined&#39; &amp;amp;&amp;amp; typeof this.q === &#39;string&#39;) {        document.location.href=route+&#39;/?query=&#39;+this.q;      } else {        return;      }    },    // returns our search query (for example, &quot;javascript tutorial&quot;)    get: function() {      return this.q;    },    // &quot;grab&quot; the query from the query string in the URL and set this.q to it    setFromURL: function(name) {      name = name.replace(/[[]/, &quot;[&quot;).replace(/[]]/, &quot;]&quot;);      var regex = new RegExp(&quot;[?&amp;amp;]&quot; + name + &quot;=([^&amp;amp;#]*)&quot;),          results = regex.exec(location.search);      this.q = results === null ? &quot;&quot; : decodeURIComponent(results[1].replace(/+/g, &quot; &quot;));      return this;            },    // a wrapper for jQuery&#39;s $.get    getJSON: function(file) {      return $.get(file);    }  };  // when we initialize a query, we have the option of giving it a query string  function Query(q) {    if(typeof q !== &#39;undefined&#39; &amp;amp;&amp;amp; typeof q === &#39;string&#39;) {      this.q = q;    }  }  // attach the Query object to the window  global.Query = Query;})(this,$);Whew! That’s a lot of stuff. Let’s write a little API documentation to show you what everything does:  var query = new Query() — we can create a new “container” to hold our search query  query.set(&#39;javascript tutorial&#39;) — this is what we want to search for, for example  query.goToLocation(&#39;my-search-page&#39;) — will bring us to /my-search-page?query=javascript%20tutorial  query.get() — returns `“javascript tutorial”, in this case  query.setFromURL() — when we reached /my-search-page?query=javascript%20tutorial, we can grab the “javascript tutorial” string, and set it (internally, it says this.q = &quot;javascript tutorial&quot;)  query.getJSON(&#39;/posts.json&#39;) — this just grabs our page, /posts.json and returns the return value of $.get (this is useful because we can call query.getJSON(&#39;/posts.json&#39;).done(function() {}))Still confused? I recommend reading up first on Immediately Invoked Function Expressions.Let the user search for something&amp;lt;!-- Your search form --&amp;gt;&amp;lt;form class=&quot;search&quot;&amp;gt; &amp;lt;input type=&quot;text&quot; class=&quot;search-box&quot; id=&quot;search&quot; /&amp;gt; &amp;lt;input type=&quot;submit&quot; class=&quot;search-button&quot; value=&quot;Search&quot; /&amp;gt;&amp;lt;/form&amp;gt;I hate forms. But here, we use them for a very specific reason. It’s so that we can execute our search function both whenever the user clicks the “Search” button, or whenever the user hits the “enter” key on the keyboard. HTML has this built-in functionality. If we didn’t use a form, and just used, say, a div, we would have to write code that would listen to both the click event on the button, and the keydown event on the text box.So on all pages where there is the above search form, we should also have this JavaScript:// search.js$(function(Query) {&#39;use strict&#39;;var query = new Query();$(&#39;.search&#39;).on(&#39;submit&#39;,submit);function submit(e) {// stop the form from doing its default behaviore.preventDefault();// set the query, and go to the search page with our query URLquery.set($(&#39;.search-box&#39;).val().trim()).goToLocation(&#39;/search&#39;);}}(Query));We could easily write the above code as such:// search.jsfunction submit(e) {e.preventDefault();document.location.href=&#39;/search/?query=&#39;+$(&#39;.search-box&#39;).val().trim();}However, the only reason we’re using our custom Query object is because it separates concerns, and we’ve also created a reusable, easy-to-read and understandable module.Finally, send the requestWe’ll have the following code on our /search page:// results.js$(function(Query,utils) {var query = new Query(),site = location.protocol + &quot;//&quot; + location.host,// some utility functionsutils = utils;query.setFromURL(&#39;query&#39;).getJSON(&#39;/posts.json&#39;).done(function(data) {console.log(data);// show our results});}(Query,utils));Remember my tip about JSONLint? Seriously, use it. Open up your console, and you should see your JSON (since we wrote console.log(data) in the above code). If you don&#39;t see it, your JSON might be improperly formatted.Wait, what’s utils?utils is a little package of a function that we’ll call shade. This function will be used to color our results based on the strength of the match against our query.// utils.js;(function(global) {&#39;use strict&#39;;var utils = {shade: shade};function shade(color,percent) {//Comes from: http://stackoverflow.com/a/13542669/2714730var f=parseInt(color.slice(1),16),t=percent&amp;lt;0?0:255,p=percent&amp;lt;0?percent*-1:percent,R=f&amp;gt;&amp;gt;16,G=f&amp;gt;&amp;gt;8&amp;amp;0x00FF,B=f&amp;amp;0x0000FF;return &quot;#&quot;+(0x1000000+(Math.round((t-R)*p)+R)*0x10000+(Math.round((t-G)*p)+G)*0x100+(Math.round((t-B)*p)+B)).toString(16).slice(1);}global.utils = utils;})(this);We’ve simply wrapped it in a module, because later on we could add more methods to module (if we wanted to). For example, a custom forEach function.Step 3 — Use lunr.js and display the resultsHere comes the fun part. First, here’s the HTML for our /search page:&amp;lt;!-- /search --&amp;gt;&amp;lt;div class=&quot;search-results-count&quot;&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;ul class=&quot;search-results&quot;&amp;gt;&amp;lt;/ul&amp;gt;That’s literally it. Our JavaScript is a bit more interesting:// results.js.done(function(data) {var searchIndex,results,$resultsCount = $(&#39;.search-results-count&#39;),$results = $(&#39;.search-results&#39;),totalScore = 0,percentOfTotal;// PIECE 1// set up the allowable fieldssearchIndex = lunr(function() {this.field(&#39;title&#39;);this.field(&#39;category&#39;);this.field(&#39;content&#39;);this.ref(&#39;url&#39;);this.field(&#39;date&#39;);});// PIECE 2// add each item from posts.json to the index$.each(data,function(i,item) {searchIndex.add(item);});// PIECE 3// search for the query and store the results as an arrayresults = searchIndex.search(query.get());// PIECE 4// add the title of each post into each result, too (this doesn&#39;t come standard with lunr.js)for(var result in results) {results[result].title = data.filter(function(post) {return post.url === results[result].ref;})[0].title;}// show how many results there were, in the DOM$resultsCount.append(results.length + (results.length === 1 ? &#39; result&#39; : &#39; results&#39;) + &#39; for &quot;&#39; + query.get() +&#39;&quot;&#39;);// PIECE 5// get the total score of all items, so that we can divide each result into it, giving us a percentage$.each(results, function(i, result) {totalScore+=result.score;});// PIECE 6 &amp;amp; PIECE 7// append each result link, with a border that corresponds to a color with a strength equal to its percentage// of the total score$.each(results, function(i,result) {percentOfTotal = result.score/totalScore;$results.append(&#39;&amp;lt;li&amp;gt;&amp;lt;a href=&quot;&#39;+ site + result.ref +&#39;&quot;&amp;gt;&#39;+result.title+&#39;&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&#39;);$results.children(&#39;li&#39;).last().css({&#39;border-left&#39;: &#39;20px solid &#39;+utils.shade(&#39;#ffffff&#39;,-percentOfTotal)});});});Let’s go over this in pieces:Piece 1We set up a searchIndex object, which is just an initialization of lunr. If you notice, we call this.field(), and every field actually exactly matches the fields that we have in our posts.json.Piece 2Then, we loop through our JSON objects from posts.json, and we add them to searchIndex.Piece 3We call .search(query.get()) on our lunr object, searchIndex. Remember, we called query.setFromURL(), so when we call query.get(), it returns the query string from the URLPiece 4It turns out that the results object only contains objects with a ref field. Open up your console and run your code and you’ll see what I mean. The ref field we set up to be the URL of the post. So all we’re gonna do is add the title of each post to the result object too, so that way later, we can add the a tags with the URLs and titles.Piece 5Each object also has a field called score, which lunr.js generates. This is a number between 0 and 1, which reflects the strength of the match. So if we have a match with a score of 0.09, and a match with a score of 0.0062, the one with 0.09 matched higher, based on lunr’s algorithm.Piece 6So, we’ll use a little bit of math. If we have two matches, result #1 at 0.09 and result #2 at 0.0062, the total is 0.0962, right? So result #1’s fraction of the total is result.score / totalScore, which is about 0.9355, and result #2’s is about 0.0644.Piece 7In comes our shade method. We’re gonna add a thick border to the side of each search result, and we’ll darken it by each result’s percentage. Thus, the higher strength of the match for a result, the darker the side border is, which shows the user, intuitively, that that specific match is “stronger”, since it has a “stronger” color. As a side note, when we loop through the elements and display them in a list with jQuery, the results are in order from highest score to lowest score by default (thanks to lunr.js), so the results are automatically ordered from high to low in your resulting HTML.ConclusionSo that’s it. We’ve successfully implemented a client-side only (or, mostly, since we use AJAX) search system.Benefits  We can use it on GitHub pages, or whatever hosting site you use that doesn’t support a backend with PHP, Node.js, etc.  Everything is written in JavaScript and HTML, so it’s relatively simple  We can render all the JSON ourselves (with Liquid and Jekyll), and serve it up statically  … It works.Drawbacks  Could be slow, depending on how many results you have. If you have a lot of results, you might want to consider using some loading icons or some sort of AJAX progress bar to show the progress of the loading so the user isn’t looking at a blank screen thinking nothing is happening. Also, you could display only a certain amount of results at a time, and wait to render to the second or third, etc., set of results until the user clicks a “next” button, or whatever.  JSON is finnicky. You have to render your JSON file with Liquid very detailed.  It’s a little bit hacky (using Liquid to make the JSON file)ExampleAs of the date of this post, I am using this on my website. Scroll down to the footer to utilize the search feature.CreditsThanks to christian-fei for the inspiration for the JSON creation via Liquid.Also, thanks Lunr.js!",
      "url"      : "/javascript/jekyll/tutorial/2015/03/26/getting-started-with-a-search-engine-for-your-site-no-server-required.html",
      "date"     : "2015-03-26 14:41:00 +0000"
    },
  
    {
      "title"    : "Tricks of the JavaScript for loop",
      "category" : "javascript",
      "content"	 : "I&#39;m sure you&#39;ve seen the common for loop written a thousand different ways. Well, in this post, you&#39;ll learn one readable way that is good for pretty much most instances where you&#39;ll use a for loop (some exceptions are noted, too). Oh, and scroll all the way down for a bonus snippet (we&#39;ll create something pretty cool, I think).We use the for loop generally to cycle through, say, an array of items. We can do something with each item. For example:var arr = [1,2,3,4,5];for(var i = 0; i &amp;lt; arr.length; i++) {console.log(arr[i]);}The code above obviously just increases i, starting at 0 and ending at the length of arr, which gives us the ability to access each of the items of arr by its index.How do you declare the loop?There’s frequently discussion on how to best write a for loop. There are some performance issues depending on how you write it (see this StackOverflow question), and I think a lot of it comes down to two things at the end: readability and personal style.Several ways:/*These will all ouput:12345*/var arr = [1,2,3,4,5];// #1for(var i = 0; i &amp;lt; arr.length; i++) {console.log(arr[i]);}// #2var i, len = arr.length;for(i = 0; i &amp;lt; len; i++) {console.log(arr[i]);}// #3var i = 0, len = arr.length;for(; i &amp;lt; len; i++) {console.log(arr[i]);}There are many other ways to write the loops, but those are some of the more common ones.Let’s go over all of them, briefly#1 — The problem with this one is that it might take some browsers longer because we’re “resetting” some variables each time (i.e., each iteration, the loop checks the section where we define the limit, which is oftentimes the length of an array). In fact, we can prove this:// #1var arr = [1,2,3,4,5];for(var i = 0; i &amp;lt; arr.length - i; i++) {console.log(arr[i]);}/*==&amp;gt; 1==&amp;gt; 2==&amp;gt; 3*/The engine appears to check each statement for every iteration of the loop. We can cache the variables, so in comes:#2 — This one caches the length, which could give you a little micro-optimized performance boost in some browsers. The first i definition doesn’t get “re-evaluated” at the end of each iteration. We can prove this too:// #2var i = 0, len = arr.length;for(i+=i; i &amp;lt; len; i++) {console.log(arr[i]);}/*==&amp;gt; 1==&amp;gt; 2==&amp;gt; 3==&amp;gt; 4==&amp;gt; 5*/As you can see above, i is only defined once. If it wasn’t, it would increase by itself each iteration, and we wouldn’t see every number of arr in the output. So it seems we’ve got to a nice, optimized way of writing the loop. But in comes the “personal style” element:#3 — I like the last one, because it defines everything at the top of the function, much like how I described as a best practice in my post about hoisting. You can see everything easier, and predict what is going to be used, and where. However, this is not the best style to use, if you need to re-define the length, or limit, every iteration (for instance, if you’re removing or adding items to an array). I think it also is very minimalistic and clean.// #3var i = 0, len = arr.length;for(; i &amp;lt; len; i++) {console.log(arr[i]);}Why does #3 not throw an error?The third one is strange, because we start it off with a ; all by its lonesome. Well, I think this can be explained if we accept that the second part of the loop, the i &amp;lt; // whatever part, looks to define itself by i, wherever i might be. The first part, the part where we normally declare and define i, it just that: a part where we can initialize variables. In fact, we could initialize any variables we wanted. We could even initialize the limit, or length variable, and the array itself! Check it out:for(var arr = [1,2,3,4,5], i = 0, len = arr.length; i &amp;lt; len; i++) {console.log(arr[i]);}Pretty cool, right? The initialization section of the loop provides us a space to intialize whatever we want; the syntactical benefit is that when a user looks at it, they know that i and len are directly tied to that loop. The downside I think, though, is that a beginner might be fooled into thinking that i only exists in the scope of the for loop, when in fact, even though i was initialized as part of the for loop, it still exists to whatever function scope that it’s inside. We can prove this:for(var arr = [1,2,3,4,5], i = 0, len = arr.length; i &amp;lt; len; i++) {console.log(arr[i]);}console.log(arr); // [1,2,3,4,5]console.log(i); // ==&amp;gt; 5console.log(len); // ==&amp;gt; 5Each of these sections (the first part where we initialize i, the second part, and the third part) all have names, by the way. They are called:for(/*initialization*/;/*condition*/;/*final-expression*/) {console.log(arr[i]);}So what have we learned?Well, we learned its easier when you call things by their names: initialization, condition, and final-expression.Initialization lessonsWe can use this to declare anything we want, but we don’t have to use it to declare anythingvar arr = [1,2,3,4,5], len = arr.length, i = 0;for(;i&amp;lt;len;i++) {console.log(arr[i]);}Additionally, although it may be a personal style issue, I think it’s best to declare everything at the top of the function where the for loop resides, unless you need to redefine something in the condition section, such as the length of an array.Condition lessonsWhen it comes to arrays, we’re probably better off caching the length of the array either in the initialization expression or at the top of the function in which the for loop resides. This can give some performance benefits. However, we want to keep in mind that in a function where we add or remove things from an array, we might need to redefine the condition expression of the for loop every iteration.Bonus!Let’s create a function that does a for loop for us. It eliminates the scoping issues of variables like i and len, and it feels a whole lot cleaner. In fact, this function is a simplified version of something that the Underscore.js and jQuery libraries do (jQuery has $.each). You can even see a very complicated polyfill for Array.prototype.forEach on MDN’s page. Here’s out simplified function:function forEach(arr,fn) {var i = 0, len = arr.length;if(arr.constructor === Array &amp;amp;&amp;amp; fn.constructor === Function) {for(;i&amp;lt;len;i++) {fn.call(arr,arr[i],i,arr);}}}// usage:var arr = [1,2,3,4,5];forEach(arr, function(element, index, array) {console.log(element);console.log(index);console.log(array);});/*==&amp;gt; 1==&amp;gt; 0==&amp;gt; [1,2,3,4,5]==&amp;gt; 2==&amp;gt; 1==&amp;gt; [1,2,3,4,5]==&amp;gt; 3==&amp;gt; 2==&amp;gt; [1,2,3,4,5]==&amp;gt; 4==&amp;gt; 3==&amp;gt; [1,2,3,4,5]==&amp;gt; 5==&amp;gt; 4==&amp;gt; [1,2,3,4,5]*/If you want to play around with that function, or just see if it works, open up your Developer Tools (cmd+opt+j on Mac in Chrome), copy and paste, hit enter, and voila.We could even put it in a module:;(function(win) {var utils = {forEach: forEach};function forEach(arr,fn) {var i = 0, len = arr.length;if(arr.constructor === Array &amp;amp;&amp;amp; fn.constructor === Function) {for(;i&amp;lt;len;i++) {fn.call(arr,arr[i],i,arr);}}}win.utils = utils;})(window);// usage:var myOtherModule = (function(utils) {var forEach = utils.forEach,arr = [1,2,3,4,5];forEach(arr,logEachElement);    /////function logEachElement(element,index,array) {console.log(element);console.log(index);console.log(array);}})(utils);Boom! Why all the strange semicolons? Why the weird function notation? Check out my post on weird semicolons and on a simple way to modularize dependencies for some answers to these question. Also, check out John Papa’s AngularJS style guide. It applies specifically to AngularJS, but a lot of the principles translate well into vanilla JavaScript principles.",
      "url"      : "/javascript/2015/03/24/tricks-of-the-javascript-for-loop.html",
      "date"     : "2015-03-24 22:18:00 +0000"
    },
  
    {
      "title"    : "How to show a summary of your post with Jekyll",
      "category" : "jekyll snippet",
      "content"	 : "In this little snippet, we&#39;re going to explore an option to show a summary of your blog post or article, if you&#39;re using the awesome static-site generator called Jekyll.I am using the static-site generator Jekyll to compile my website into a bunch of static files (the main benefit being a generally more secure and potentially quicker website due to a lack of server round-trips to a database).I ran across several solutions for displaying a summary or excerpt of the post in the main blog page (if you’re using the basic Jekyll theme that comes standard, it’ll probably be your index.html).Anyway, the solution I came up with is meant to allow me to control what part of the post I want to display as an excerpt. If I don’t choose a part of the post, that’s okay, it just displays a default excerpt.If you didn’t know, Jekyll uses Liquid for its templating.Example:&amp;lt;!-- index.html --&amp;gt;&amp;lt;p class=&quot;post-excerpt&quot;&amp;gt;{% if post.content contains &#39;&amp;lt;!--excerpt.start--&amp;gt;&#39; and post.content contains &#39;&amp;lt;!--excerpt.end--&amp;gt;&#39; %}{{ ((post.content | split:&#39;&amp;lt;!--excerpt.start--&amp;gt;&#39; | last) | split: &#39;&amp;lt;!--excerpt.end--&amp;gt;&#39; | first) | strip_html | truncatewords: 20 }}{% else %}{{ post.content | strip_html | truncatewords: 20 }}{% endif %}&amp;lt;/p&amp;gt;&amp;lt;!-- _posts/some-random-post.html --&amp;gt;&amp;lt;p&amp;gt;Here&#39;s all my content, and &amp;lt;!--excerpt.start--&amp;gt;here&#39;s where I want my summary to begin, and this is where I want it to end&amp;lt;!--excerpt.end--&amp;gt;.&amp;lt;/p&amp;gt;If I don’t add the comments in the post, the template with simply extract the content of the post, strip the HTML tags, and truncate it 20 words, followed by an ellipsis ....",
      "url"      : "/jekyll/snippet/2015/03/23/how-to-show-a-summary-of-your-post-with-jekyll.html",
      "date"     : "2015-03-23 09:50:00 +0000"
    },
  
    {
      "title"    : "Why do people add semicolons before modules?",
      "category" : "javascript snippet",
      "content"	 : "This post is going to discuss the reasoning behind a strange-looking syntax style that some people use when declaring JavaScript modules (in the context of immediately invoked function expressions). Simply put, the point of this trick is to get around minification issues when using other people&#39;s code (or your own).Simple answer: because of minification issues.Minification can cause modules to use each other as their arguments (unintentionally), if the developer isn’t careful.(function() {// code})()(function() {// code})();If you look closely enough at the above, you’ll see the first IIFE is missing a semicolon at the end.That means that when minified, it’ll look like this:The broken code:(function() {})()(function() {})();// (a)()(b)()The problem is that now function a is being called with function b passed in as an argument. Interesting.So, we just add a ; to the beginning of the module, and to the end. This acts as a safeguard to ensure we don’t run into that problem when we minify.So, when we try doing the above example with semicolons at the beginning and at the end, and you minify the code, you get this instead:The working code:;(function() {})();;(function() {})();The cool thing is, the above doesn’t throw any errors. In fact, JSHint won’t yell at you either.Why is this? Check out the MDN article on empty:  An empty statement is used to provide no statement, although the JavaScript syntax would expect one.So those extra semicolons are not a syntax error, because a random semicolon anywhere in the code can be interpreted as an empty statement. Cool, huh?",
      "url"      : "/javascript/snippet/2015/03/22/why-do-people-add-semicolons-before-modules.html",
      "date"     : "2015-03-22 23:34:00 +0000"
    },
  
    {
      "title"    : "Lightweight dependency modularization",
      "category" : "javascript",
      "content"	 : "So what&#39;s this post all about? We&#39;re going to examine an interesting alternative for passing dependencies between JavaScript modules. The goal is to avoid any overhead (by using libraries like RequireJS), but also to avoid attaching too many things to the global namespace...A common approachA common approach when utilizing the module pattern or something close to it, is to attach the end object to the window, or return the object inside some sort of local variable to create a namespace for your module.So you might end up having something like this:// first.module.js;(function(win) {var exports = //codewin.firstModule = exports;})(window);// several more modules...// and eventually a module that depends on firstModule// fifth.module.js;(function(win,firstModule) {// do something with firstModulevar exports = //code;win.fifthModule = exports;})(window,firstModule);&amp;lt;!-- then in your html... --&amp;gt;&amp;lt;script src=&quot;first.module.js&quot;&amp;gt;&amp;lt;/script&amp;gt;&amp;lt;!-- several other modules --&amp;gt;&amp;lt;scirpt src=&quot;fifth.module.js&quot;&amp;gt;&amp;lt;/script&amp;gt;Of course, the obvious problems with this are:  You attach a lot of stuff to the global scope (this can be overcome with various techniques, though)  You have to get the order of your &amp;lt;script&amp;gt; tags pretty exact (we’re pretending we’re not using a task runner here, like gulp)… and probably several other I’m forgetting about. There are other ways to do this (a popular library like RequireJS, for example), but in my case, I wanted to try to use a basic object that would be attached to the window, and have all of my dependencies attach to that global object.A global dependencies objectFor angular-state-manager, I utilized a global object called stateManagerDependencies, or something along that line, to attach all of my modules to that would eventually go into the main stateManager module.The purpose of this was to only have one object (the stateManagerDependencies object) be attached to the global scope (i.e. window object), and that would hold all of the dependencies, as opposed to attaching each dependency to the global scope at the end of each module.// dependencies.js;(function(win) {win.stateManagerDependencies = {};})(window);// first.example.module.js;(function(dependencies) {var dependencies = dependencies;// then, use them...})(stateManagerDependencies);// etc.Obvious downsides to this approach  You’re still attaching some strangely named global object to the window  In each module that depends on the dependencies object, there is no error-checking or method to list what modules are available inside the dependencies object (a possible fix for this is to simply implement some of those methods, for example: .list(), or .checkFor(), etc.)",
      "url"      : "/javascript/2015/03/22/lightweight-dependency-modularization.html",
      "date"     : "2015-03-22 21:00:00 +0000"
    },
  
    {
      "title"    : "How do I check if a parameter was passed in to a function in JavaScript?",
      "category" : "javascript",
      "content"	 : "This post is going to talk about checking for the &quot;existence&quot; of a parameter, if we expected one to be there and how to handle this &quot;flow of control&quot;, and how we define &quot;checking for existence&quot; in the first place. The fundamental concept behind all of this, as we&#39;ll learn, is the fuzzy idea of truthiness (which is different depending on which programming language you&#39;re referring too.) We&#39;ll see that JavaScript has a very broad idea of truthiness.What’s a parameter?A parameter can also be called an argument. It’s the thing passed into a function that you do something with:function test(x) {return x + 1;}test(1); // ==&amp;gt; returns 2var two = test(1); // this variable is now set to 2But what if a function can do something without a parameter, or what if the functionality changes based on whether or not a parameter was passed in?function test(x) {if(x) {// do something} else {// do something else}}In the above example, we expect that if x “exists”, then we’ll do something; if not, we’ll do something else. But checking for existence is a little more difficult than that. It has to do with truthiness.The truth about truthinessIn if statements, JavaScript evaluates the statement to a boolean true or false, and acts accordingly. But when certain values are encountered, JavaScript “coerces” them to a boolean value. For example, these statements are all correct:These are all true:/*1 == true0 == false&#39; &#39; == true&#39;&#39; == falsenull == falseundefined == false*/There are plenty of other examples. However, you’ll notice above that we’re using the == operator. This is called the equality operator. It checks if two values are “equal”, but to do this, JavaScript coerces the types.These are all incorrect (they will evaluate to false):/*1 === true0 === false&#39; &#39; === true&#39;&#39; === falsenull === falseundefined === false*/You’ll notice a very subtle, but important, difference here: we’re using the === operator: also called the identity operator. 1 does not have the identity of true, however, it is equal to true. This is confusing at first, but this is the principle of truthiness.This brings us to a very important logical operator, the exlamation point ! (also called the “not” operator). It gives you the opposite of whatever boolean value you give.The “not” operator in action (the following statements are all true)/*!1 === false!0 === true!&#39; &#39; === false!&#39;&#39; === true!null === true!undefined === true*/You’ll notice above that we are using the strict identity operator ===. This is okay, because the ! operator coerces the values on the left side of the equation to a boolean value. So really when we say !1 === false, we’re actually saying false === false. In other words, false does have the identity of false.The “double not” (or just two “not” operators — these are also all true)/*!!1 === true!!0 === false!!&#39; &#39; === true!!&#39;&#39; === false!!null === false!!undefined === false*/The double not basically coerces each value into its equal boolean form. That’s a weird way to say it. Think of it like this: 0 == false is the same as !!0 === false.So how do we check for existence?Well, it depends on how we want to define existence. Do we want to say that the parameter doesn’t exist, if it is null,undefined,0,false, or some other non-truthy value? What if we want to pass in an empty string (&#39;&#39;) as a value?Bad example:function test(x) {if(!!x) {console.log(&#39;we passed the &quot;if&quot; test&#39;);console.log(&#39;empty strings are okay too&#39;);}}test(null); // logs nothingtest(1); // ==&amp;gt; &#39;we passed the &quot;if&quot; test&#39;; &#39;empty strings are okay too&#39;test(&#39;&#39;); // logs nothing... uh oh// By the way, if(!!x) is basically shorthand for if(x == true)So we can combine instead combine logical statements to check for our idea of existence:Better example:function test(x) {if(!!x || x === &#39;&#39;) {console.log(&#39;we passed the &quot;if&quot; test&#39;);console.log(&#39;empty strings are okay too&#39;);}}test(null); // logs nothingtest(1); // ==&amp;gt; &#39;we passed the &quot;if&quot; test&#39;; &#39;empty strings are okay too&#39;test(&#39;&#39;); // ==&amp;gt; &#39;we passed the &quot;if&quot; test&#39;; &#39;empty strings are okay too&#39;// We could even use &#39;typeof&#39;function test(x) {if(!!x || typeof x === &#39;string&#39;) {console.log(&#39;we passed the &quot;if&quot; test&#39;);console.log(&#39;empty strings are okay too&#39;);}}// the above function will output the same thingChecking strictly for undefined values:function test() {if(a) {console.log(&#39;a exists!&#39;);}}// The above function will throw a ReferenceError: a is not definedfunction test() {if(typeof a !== &#39;undefined&#39;)  {console.log(&#39;a exists!&#39;);}}// This second function won&#39;t log anything, but it also won&#39;t throw an error!In the above example, a was never declared. If a variable is never declared and you try to check for its existence, you’ll get an error. But the typeof operator is the only operator (to my knowledge) that gets around this. If you check the type of a variable that was never declared, you won’t get an error, and your program can continue.Declared vs. definedvar a; // this is a declared variable that exists; its value is undefinedvar a = true; // this is now a defined variable; its value is NO LONGER undefinedvar b;console.log(typeof b); // ==&amp;gt; undefinedconsole.log(!!b); // ==&amp;gt; falsevar c;console.log(typeof c); // ==&amp;gt; undefinedconsole.log(!!c); // ==&amp;gt; ReferenceError! Your program is dead.Best PracticeI think in this circumstance, the best practice depends on however you define “existence” in the context of your specific function. This means that you have to take into consideration the concepts of truthiness, the use of the typeof operator, the principle of declared vs. defined, etc.Keep in mind, too, that there are many ways to check the types of of variables you encounter. For example:var x = [];// x.constructor === Array// typeof x === &#39;object&#39;// x.toString() === &#39;&#39;var y = &#39;hello&#39;;// y.constructor === String// typeof y === &#39;string&#39;// y.toString() === &#39;hello&#39;This is a topic for another post, but it demonstrates how you can check the types of a variable based on your definition of existence in the context of your specific function.",
      "url"      : "/javascript/2015/03/20/how-do-i-check-if-a-parameter-was-passed-in-to-a-javascript-function.html",
      "date"     : "2015-03-20 22:39:00 +0000"
    },
  
    {
      "title"    : "What is hoisting in JavaScript?",
      "category" : "javascript",
      "content"	 : "What is this strange word &quot;hoisting&quot;? As of the current version of JavaScript (ECMAScript5), there&#39;s not really such a thing as block scope, which is something common to lots of other programming languages (there is a caveat though, which we&#39;ll learn towards the end of this post.) Hoisting and the non-existence of &quot;block scope&quot; can be confusing. Learn how to overcome this issue.Hoisting is small concept that’s pretty importantHoisting is a concept that really defines a fundamental principle of JavaScript: there’s no such thing as block scope.What does that mean? Well, we know there’s a thing called function scope:Function scope example// This area out here is our &quot;global scope&quot;var globalVariable = [0,1,2,3];function testFunction() {//This area in here is our &quot;function scope&quot;// We can access variables from the global scopeconsole.log(globalVariable) // ==&amp;gt; [0,1,2,3]// &quot;var&quot; makes this variable only exists inside the functionvar hello = &#39;hello&#39;;// since there&#39;s no &quot;var&quot; keyword,// we might as well have put this variable in the global scopeanotherVariable = &#39;anotherVariable&#39;; }console.log(hello) // ==&amp;gt; undefinedconsole.log(anotherVariable) // ==&amp;gt; &#39;anotherVariable&#39;… Easy.But how do we define block scope? In some other languages, it might be whatever is inside an if statement or a for loop. But that doesn’t exist in JavaScript (well, with a caveat, which we’ll go over in a later section):If-statement examplefunction testFunction(num) {if(num &amp;lt; 5) {console.log(&#39;less than five&#39;);} else {console.log(&#39;greater than or equal to five&#39;);}}testFunction(4); // ==&amp;gt; &#39;less than five&#39;The above example is super easy to understand. But let’s try and do something else with it.Trying to use block scope in JavaScriptfunction test() {if(/* something */) {var foo = &#39;bar&#39;;console.log(&#39;foo&#39;); // ==&amp;gt; &#39;bar&#39;}console.log(&#39;foo&#39;); // ==&amp;gt; &#39;bar&#39;}In the above example, if you didn’t know any better and you thought JavaScript had block scope, you might assume that the variable foo only exists inside the if block. But it doesn’t. In fact, even if the condition in the if statement evaluated to false, the variable foo would still exist to the entire function, but it just would not be set to &#39;bar&#39; (it would actually just remain undefined).So what’s hoisting then?Anything defined inside of a block is actually “hoisted” up to the function scope of whatever function you’re in.function test() {if(/* something */) {var foo;}}// it gets evaluated to something like this:function test() {var foo;if(/* something */) {// do something}}Anything you declare inside of a block (whether it’s a function, a string, an array, or whatever), it is available to the entire scope. Blocks simply evaluate those variable to whatever you say, when a certain condition is met.Best PracticeIn my opinion, declaring all variables that will, or even just might, be later defined within your function should happen at the top of the function. Some arguments against this might cite possible performance issues, but that might be going into the arguments of micro-optimization vs readability.  In my opinion, declaring all variables that will, or even just might, be later defined within your function should happen at the top of the functionI humbly assert this opinion because I think it helps immensely with code readability, especially with large, wordy functions, and understanding ahead of time what is going to happen and what certain variable might get new definitions.function someFunction() {var test, test1, test2, test3;if(/* something */) {test = 0;} else if (/* something */) {test1 = 0;} else if (/* something */) {test2 = 0;} else if (/* something */) {test3 = 0;}}In the above example, you know all the variables ahead of time, even though only one of them might actually be set to something else other than undefined (in this case, only one of those variables might be set to 0).To me, this looks like it could get confusing:function someFunction() {if(/* something */) {var test = 0;} else if (/* something */) {var test1 = 0;} else if (/* something */) {var test2 = 0;} else if (/* something */) {var test3 = 0;}}While this is a small, manufactured function that probably doesn’t make sense and probably does a whole lot of nothing, in a lengthier function, someone else who reads the code might see a new variable definition, and have trouble understanding the context or the use. Additionally, all those variable declarations will get hoisted to the top of the function anyway. You’ll still have three variables that are undefined, and only one that is set to 0, but the code just happens to be more confusing.Small caveat regarding letIf you use the keyword let in place of var, everything I just wrote in this blog post is null and void. let allows you to declare and/or define a block-scoped variable (see the MDN page, which says “The let statement declares a block scope local variable, optionally initializing it to a value.”)Note this warning on the MDN page as well:  The let block and let expression syntax is non-standard and will be removed in the future. Do not use them!",
      "url"      : "/javascript/2015/03/20/what-is-hoisting-in-javascript.html",
      "date"     : "2015-03-20 17:31:00 +0000"
    }
  
]